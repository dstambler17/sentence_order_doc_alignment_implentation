{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTHONPATH=\"/home/dstambl2/doc_alignment_implementations/thompson_2021_doc_align\"\n"
     ]
    }
   ],
   "source": [
    "%set_env PYTHONPATH=\"/home/dstambl2/doc_alignment_implementations/thompson_2021_doc_align\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dstambl2/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "## Standard Library\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import defaultdict, namedtuple\n",
    "from random import shuffle, randint\n",
    "\n",
    "## External Libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as functional\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINE Training Hyperparams And Constants\n",
    "\n",
    "## Batch Size\n",
    "TRAIN_BATCH_SIZE = 32\n",
    "VAL_BATCH_SIZE = 32\n",
    "\n",
    "## Learning Rate\n",
    "LR = 0.001\n",
    "\n",
    "# Epochs (Consider setting high and implementing early stopping)\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "GPU_BOOL = torch.cuda.is_available()\n",
    "GPU_BOOL\n",
    "\n",
    "BASE_DATA_PATH=\"/home/dstambl2/doc_alignment_implementations/data\"\n",
    "BASE_EMBED_DIR = '/home/dstambl2/doc_alignment_implementations/data/cc_aligned_si_data/embeddings'\n",
    "\n",
    "BASE_PROCESSED_PATH=\"/home/dstambl2/doc_alignment_implementations/data/cc_aligned_si_data/processed\" \n",
    "ALIGNED_PAIRS_DOC = '/home/dstambl2/doc_alignment_implementations/data/cc_aligned_en_si.pairs'\n",
    "\n",
    "SRC_LANG_CODE=\"en\"\n",
    "TGT_LANG_CODE=\"si\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73 11 21\n"
     ]
    }
   ],
   "source": [
    "#Split into train, valid and test sets\n",
    "\n",
    "from utils.common import load_extracted, map_dic2list, \\\n",
    "    filter_empty_docs, regex_extractor_helper, tokenize_doc_to_sentence\n",
    "from modules.get_embeddings import read_in_embeddings, load_embeddings\n",
    "from modules.build_document_vector import build_document_vector\n",
    "from modules.vector_modules.boiler_plate_weighting import LIDFDownWeighting\n",
    "from align_docs import fit_pca_reducer\n",
    "from modules.vector_modules.window_func import ModifiedPertV2\n",
    "from utils.lru_cache import LRUCache\n",
    "from utils.common import function_timer\n",
    "\n",
    "CHUNK_RE = re.compile(r'(chunk_\\d*(?:_\\d*)?)', flags=re.IGNORECASE)\n",
    "BASE_DOMAIN_RE = re.compile(r'https?\\://(?:w{3}\\.)?(?:(?:si|en)\\.)?(.*?)/', flags=re.IGNORECASE)\n",
    "\n",
    "#from sklearn.model_selection import train_test_split\n",
    "embed_chunk_paths = []\n",
    "for subdir, dirs, files in os.walk(BASE_EMBED_DIR):\n",
    "    if subdir != BASE_EMBED_DIR:\n",
    "        embed_chunk_paths.append(subdir)\n",
    "        \n",
    "embed_chunk_paths = sorted(embed_chunk_paths, key= lambda x: len(x))\n",
    "\n",
    "#For now split into 0.7/0.1/0.2 split\n",
    "#Split into train, test and val sets, remove last one for split since it will go into train set\n",
    "train_ind, test_ind = train_test_split(list(range(len(embed_chunk_paths[:-1]))), test_size=0.2, random_state=1)\n",
    "train_ind, val_ind = train_test_split(train_ind, test_size=0.125, random_state=1) # 0.125 x 0.8 = 0.1\n",
    "\n",
    "train_ind.append(len(embed_chunk_paths) -1) #Add last imbalenced idx to train set\n",
    "\n",
    "assert not any(set(train_ind) & set(test_ind)) and \\\n",
    "       not any(set(train_ind) & set(val_ind))and \\\n",
    "       not any(set(test_ind) & set(val_ind))\n",
    "\n",
    "print(len(train_ind), len(val_ind), len(test_ind))\n",
    "\n",
    "chunks_paths_train = [embed_chunk_paths[t] for t in train_ind]\n",
    "chunks_paths_val = [embed_chunk_paths[t] for t in val_ind]\n",
    "chunks_paths_test = [embed_chunk_paths[t] for t in test_ind]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 3, 4, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 34, 36, 40, 41, 42, 43, 44, 45, 48, 49, 50, 51, 52, 54, 55, 57, 60, 61, 62, 63, 64, 66, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 82, 83, 86, 87, 90, 92, 93, 94, 95, 97, 99, 102, 104]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(train_ind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' The following two funcs are only for getting sent embeds '''\n",
    "def get_base_embedding(url, embedding_file_path, lang_code):\n",
    "\n",
    "    chunk = regex_extractor_helper(CHUNK_RE, embedding_file_path)\n",
    "    doc_path = '%s/%s.%s.gz' % (BASE_PROCESSED_PATH, chunk, lang_code)\n",
    "    url_doc_dict = filter_empty_docs(load_extracted(doc_path))\n",
    "    if url not in url_doc_dict:\n",
    "        print(\"missing url in get_base_embedding %s for doc path %s\" % (url, doc_path))\n",
    "        #return noise\n",
    "        return np.random.random((1,1024))\n",
    "    doc_text = url_doc_dict[url]\n",
    "        \n",
    "    _, sent_embeds = read_in_embeddings(doc_text, embedding_file_path, lang_code)\n",
    "    \n",
    "    return sent_embeds\n",
    "\n",
    "\n",
    "def load_embed_pairs(src_url, tgt_url, embed_dict,\n",
    "                    src_lang=SRC_LANG_CODE,\n",
    "                    tgt_lang=TGT_LANG_CODE):\n",
    "    src_path = embed_dict[src_url]\n",
    "    tgt_path = embed_dict[tgt_url]\n",
    "    line_embeddings_src = get_base_embedding(src_url, src_path, src_lang)\n",
    "    line_embeddings_tgt = get_base_embedding(tgt_url, tgt_path, tgt_lang)\n",
    "    \n",
    "    #print(line_embeddings_src.shape, line_embeddings_tgt.shape)\n",
    "    return line_embeddings_src, line_embeddings_tgt\n",
    "''' END OF PURE SENT EMBED FUNCS '''\n",
    "\n",
    "def get_matching_url_dicts(input_path = ALIGNED_PAIRS_DOC):\n",
    "    src_to_tgt = {}\n",
    "    tgt_to_src = {}\n",
    "    with open(input_path, 'r') as fp:\n",
    "        for row in fp:\n",
    "            src, tgt = row.split('\\t')\n",
    "            src_to_tgt[src] = tgt\n",
    "            tgt_to_src[tgt]  = src\n",
    "    return src_to_tgt, tgt_to_src\n",
    "\n",
    "\n",
    "def load_embed_dict(chunks_paths):\n",
    "    embed_dict = {}\n",
    "    \n",
    "    for chunk_path in chunks_paths:\n",
    "        embed_dict_path = '%s/embedding_lookup.json' % (chunk_path)\n",
    "        embed_dict_chunk = {}\n",
    "        with open(embed_dict_path, 'r') as f:\n",
    "            embed_dict_chunk = json.load(f)\n",
    "        embed_dict.update(embed_dict_chunk)\n",
    "                \n",
    "    return embed_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Sample Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CandidateTuple = namedtuple(\n",
    "    \"CandidateTuple\", \"src_embed_path, tgt_embed_path, src_url, tgt_url, y_match_label\")\n",
    "\n",
    "\n",
    "''' Sampling logic start'''\n",
    "@function_timer\n",
    "def create_positive_samples(embed_dict, src_to_tgt_map, tgt_to_src_map, data_list):\n",
    "    '''\n",
    "    Builds positive samples\n",
    "    '''\n",
    "    for src_url, tgt_url in src_to_tgt_map.items():\n",
    "        src_url, tgt_url = src_url.strip(), tgt_url.strip()\n",
    "        if src_url in embed_dict and tgt_url in embed_dict:\n",
    "            src_embed_path, tgt_embed_path = embed_dict[src_url], embed_dict[tgt_url]\n",
    "            c = CandidateTuple(src_embed_path, tgt_embed_path, src_url, tgt_url, 1)\n",
    "            data_list.append(c)\n",
    "\n",
    "\n",
    "def create_all_possible_neg_pairs(src_to_tgt_map, tgt_to_src_map):\n",
    "    \n",
    "    src_url_list, tgt_url_list = list(src_to_tgt_map.keys()), list(tgt_to_src_map.keys())\n",
    "    domain_dict = defaultdict(lambda: defaultdict(list)) #Ex{dom: {src: [], tgt: []}}\n",
    "    for url in tgt_url_list:\n",
    "        #url = url.strip()\n",
    "        base_url = regex_extractor_helper(BASE_DOMAIN_RE, url).strip()\n",
    "        domain_dict[base_url]['tgt'].append(url)\n",
    "    \n",
    "    for url in src_url_list:\n",
    "        #url = url.strip()\n",
    "        base_url = regex_extractor_helper(BASE_DOMAIN_RE, url).strip()\n",
    "        domain_dict[base_url]['src'].append(url)\n",
    "    \n",
    "    negative_sample_dict = {}\n",
    "    #Loop through all domains and create final negative pairing\n",
    "    for domain, values in domain_dict.items():\n",
    "        sample_list = []\n",
    "        src_urls, tgt_urls = values['src'], values['tgt']\n",
    "        \n",
    "        if len(src_urls) > 100:\n",
    "            shuffle(src_urls)\n",
    "        if len(tgt_urls) > 100:\n",
    "            shuffle(tgt_urls)\n",
    "        for src_url in src_urls[:min(100, len(src_urls))]:\n",
    "            for tgt_url in tgt_urls[:min(100, len(src_urls))]:\n",
    "                if src_to_tgt_map[src_url] != tgt_url and tgt_to_src_map[tgt_url] != src_url:\n",
    "                    sample_list.append((src_url, tgt_url))\n",
    "        if len(sample_list) > 0:\n",
    "            negative_sample_dict[domain] = sample_list\n",
    "    return negative_sample_dict\n",
    "        \n",
    "    \n",
    "\n",
    "def same_domain_neg_sample_helper(negative_sample_dict):\n",
    "    '''\n",
    "    IDEA: randomly modify docs  \n",
    "    Helper function for returning\n",
    "    Negative domains of same idx\n",
    "\n",
    "    Algo: 1) Pick random domain\n",
    "         2) Pick random sample from that domain\n",
    "         3) pop that sample\n",
    "    '''\n",
    "    domain_list = list(negative_sample_dict.keys())\n",
    "    domain = domain_list[randint(0, len(domain_list) - 1)]\n",
    "    \n",
    "    neg_pair_list = negative_sample_dict[domain]\n",
    "    neg_pair_idx = randint(0, len(neg_pair_list) - 1)\n",
    "    src_url, tgt_url = neg_pair_list[neg_pair_idx]\n",
    "    \n",
    "    #Update the dict to remove neg pair\n",
    "    neg_pair_list.pop(neg_pair_idx)\n",
    "    if len(neg_pair_list) == 0:\n",
    "        negative_sample_dict.pop(domain)\n",
    "    else:\n",
    "        negative_sample_dict[domain] = neg_pair_list\n",
    "\n",
    "    return src_url, tgt_url\n",
    "\n",
    "\n",
    "def create_negative_samples(embed_dict, src_to_tgt_map, tgt_to_src_map, data_list):\n",
    "    '''\n",
    "    Builds negative samples\n",
    "    '''\n",
    "\n",
    "    number_pos_samples = len(data_list)\n",
    "    visited_urls, MAX_INSTANCES_ALLOWED = defaultdict(int), 10 \n",
    "    \n",
    "    negative_sample_dict = create_all_possible_neg_pairs(src_to_tgt_map, tgt_to_src_map)    \n",
    "    \n",
    "    \n",
    "    print(\"URL LIST LENGTHS\", number_pos_samples, len(embed_dict), len(src_to_tgt_map), len(tgt_to_src_map))\n",
    "    \n",
    "    loop_counter = 0 #NOTE: FOR DEBUG\n",
    "    for i in range(number_pos_samples):\n",
    "        \n",
    "        while True:\n",
    "            src_url, tgt_url = same_domain_neg_sample_helper(negative_sample_dict)\n",
    "\n",
    "            #Repeat until all conditions are met\n",
    "            if (visited_urls[src_url.strip()] < MAX_INSTANCES_ALLOWED and \\\n",
    "                visited_urls[tgt_url.strip()] < MAX_INSTANCES_ALLOWED and \\\n",
    "                src_to_tgt_map[src_url] != tgt_url and tgt_to_src_map[tgt_url] != src_url):\n",
    "                \n",
    "                src_url, tgt_url = src_url.strip(), tgt_url.strip()\n",
    "                if src_url in embed_dict and tgt_url in embed_dict:\n",
    "                    src_embed_path, tgt_embed_path = embed_dict[src_url], embed_dict[tgt_url]\n",
    "                    c = CandidateTuple(src_embed_path, tgt_embed_path, src_url, tgt_url, 0)\n",
    "                    data_list.append(c)\n",
    "                \n",
    "                    visited_urls[src_url] += 1\n",
    "                    visited_urls[tgt_url] += 1\n",
    "                    \n",
    "                break\n",
    "            else:\n",
    "                loop_counter += 1\n",
    "\n",
    "    print(\"LOOP COUNTER LEN %s\" % loop_counter)\n",
    "    print(len(data_list))\n",
    "        \n",
    "\n",
    "def build_pair_dataset(embed_dict, src_to_tgt_map, tgt_to_src_map):\n",
    "    '''\n",
    "    Create positive and negative url tuple pairs\n",
    "    '''\n",
    "    data_list = []\n",
    "    create_positive_samples(embed_dict, src_to_tgt_map, tgt_to_src_map, data_list) #First Create positive samples\n",
    "    print(len(data_list))\n",
    "    \n",
    "    subset_src_to_tgt_map = {k: v for k,v in src_to_tgt_map.items() if k.strip() in embed_dict and v.strip() in embed_dict}\n",
    "    subset_tgt_to_src_map = {k: v for k,v in tgt_to_src_map.items() if k.strip() in embed_dict and v.strip() in embed_dict}\n",
    "    \n",
    "    create_negative_samples(embed_dict, subset_src_to_tgt_map, subset_tgt_to_src_map, data_list)  #Now create negative samples\n",
    "    \n",
    "    #Shuffle and return data\n",
    "    shuffle(data_list)\n",
    "    print(len(data_list))\n",
    "\n",
    "    return data_list\n",
    "\n",
    "en_to_si_pairs, si_to_en_pairs = get_matching_url_dicts()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function create_positive_samples took 0.33 seconds\n",
      "45606\n",
      "URL LIST LENGTHS 45606 90001 45606 44260\n",
      "LOOP COUNTER LEN 75\n",
      "91212\n",
      "91212\n",
      "https://tattoosartideas.com/tiger-tattoo/ https://www.ideabeam.com/mobile/store/idealz-lanka\n"
     ]
    }
   ],
   "source": [
    "#TEST ABOVE CODE\n",
    "embed_dict_train = load_embed_dict(chunks_paths_train)\n",
    "data_list = build_pair_dataset(embed_dict_train, en_to_si_pairs, si_to_en_pairs)\n",
    "sample_one = list(embed_dict_train.keys())[10]\n",
    "sample_two = list(embed_dict_train.keys())[57]\n",
    "print(sample_one, sample_two)\n",
    "#load_embed_pairs(sample_one, sample_two, embed_dict_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper logic (PCA/Cache) for Doc Embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn.decomposition import PCA\n",
    "def fit_pca_reducer_debug(embedding_list_src, embedding_list_tgt):\n",
    "    '''\n",
    "    Builds PCA Dim Reduction from sample of sentence embeddings\n",
    "    in the webdomain\n",
    "    '''\n",
    "    all_sent_embeds = np.vstack(embedding_list_src + embedding_list_tgt)\n",
    "\n",
    "    pca = PCA(n_components=128)\n",
    "    divide_num = 1\n",
    "    if len(all_sent_embeds) // 6 >= 128:\n",
    "        divide_num = 6\n",
    "    elif len(all_sent_embeds) // 5 >= 128:\n",
    "        divide_num = 5\n",
    "    elif len(all_sent_embeds) // 4 >= 128:\n",
    "        divide_num = 4\n",
    "    elif len(all_sent_embeds) // 3 >= 128:\n",
    "        divide_num = 3\n",
    "    elif len(all_sent_embeds) // 2 >= 128:\n",
    "        divide_num = 2\n",
    "    elif len(all_sent_embeds) // 1 >= 128:\n",
    "        divide_num = 1\n",
    "    else:\n",
    "        sent_size = all_sent_embeds.shape[0]\n",
    "        num_iters = int(math.ceil(128 / sent_size))        \n",
    "        all_sent_embeds = np.repeat(all_sent_embeds, repeats=num_iters, axis=0)\n",
    "        \n",
    "\n",
    "    my_rand_int = np.random.randint(all_sent_embeds.shape[0], size=len(all_sent_embeds) // divide_num)\n",
    "    pca_fit_data = all_sent_embeds[my_rand_int, :]\n",
    "    pca.fit(pca_fit_data)\n",
    "    return pca\n",
    "\n",
    "#DEFINE Helper functions for building doc vectors\n",
    "#NOTE: Much of this info will be stored in an LRU cache\n",
    "\n",
    "\n",
    "class CachedData:\n",
    "    '''\n",
    "    Keeps organized cache of data\n",
    "    Keys will be domain_name\n",
    "    Since for each domain, we want the source and target lang info\n",
    "    '''\n",
    "    def __init__(self, src_text_list_tokenized,\n",
    "                       src_embed_list,\n",
    "                       src_url_list,\n",
    "                       tgt_text_list_tokenized,\n",
    "                       tgt_embed_list,\n",
    "                       tgt_url_list,\n",
    "                       ):\n",
    "        self.src_text_list_tokenized = src_text_list_tokenized\n",
    "        self.src_embed_list = src_embed_list\n",
    "        self.src_url_list = src_url_list\n",
    "\n",
    "        self.tgt_text_list_tokenized = tgt_text_list_tokenized\n",
    "        self.tgt_embed_list = tgt_embed_list\n",
    "        self.tgt_url_list = tgt_url_list\n",
    "        \n",
    "        self.lidf_weighter = LIDFDownWeighting(src_text_list_tokenized + tgt_text_list_tokenized)\n",
    "        self.pca = fit_pca_reducer_debug(src_embed_list, tgt_embed_list)\n",
    "    \n",
    "    def get_fitted_objects(self):\n",
    "        '''\n",
    "        Return PCA and LIDF\n",
    "        '''\n",
    "        return self.pca, self.lidf_weighter\n",
    "\n",
    "    def get_src_data(self):\n",
    "        return self.src_text_list_tokenized, self.src_embed_list, self.src_url_list\n",
    "    \n",
    "    def get_tgt_data(self):\n",
    "        return self.tgt_text_list_tokenized, self.tgt_embed_list, self.tgt_url_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain Specific Doc Embedding Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_embeds_for_domain(embed_dict, lang_code, text_list, url_list):\n",
    "    '''\n",
    "    Load in embeds for a domain\n",
    "    ''' \n",
    "    embed_list = []\n",
    "    try:\n",
    "        for ii in range(len(text_list)):\n",
    "            url, doc_text = url_list[ii], text_list[ii]\n",
    "            if url in embed_dict: #TEMP FIX: TODO: Rerun embeds and issue of missing URLS here or delete sentences that miss embeds\n",
    "                embed_file_path = embed_dict[url]\n",
    "                try:\n",
    "                    _, embeddings = read_in_embeddings(doc_text, embed_file_path, lang_code)\n",
    "                    embed_list.append(embeddings)\n",
    "                except:\n",
    "                    print(embed_file_path, lang_code, \"EMBED EXCEPTION\")\n",
    "            else:\n",
    "                print(\"URL NOT IN EMBED DICT: \", url)\n",
    "\n",
    "    except KeyError as e: #For debugging\n",
    "        print(e)\n",
    "        print(\"EXCEPTION OCCURED in load_embeds_for_domain\")\n",
    "    \n",
    "    return embed_list\n",
    "\n",
    "\n",
    "''' Domain Specific Chunk helper data'''\n",
    "\n",
    "def get_all_chunks_with_domain(embed_dict, base_domain):\n",
    "    '''\n",
    "    First get a list of all chunks that contain the domain_name\n",
    "    Second get all docs in src lang and tgt lang\n",
    "    Third, get pca, ldf weighter and more\n",
    "    '''\n",
    "    chunks = [regex_extractor_helper(CHUNK_RE, value) \\\n",
    "                for key, value in embed_dict.items() if base_domain.strip() in key.strip().lower()]\n",
    "    if len(chunks) == 0:\n",
    "        print(\"WAIT, CHUNK LIST IS EMPTY, so CHUNK_RE error\")\n",
    "    return list(set(chunks))\n",
    "\n",
    "def load_all_chunks_for_domain(chunk_list, base_domain, lang_code):\n",
    "    '''\n",
    "    Given a list of domain chunks and the domain_name\n",
    "    Get a domain doc dict\n",
    "    '''\n",
    "    domain_doc_dict = {}\n",
    "    for chunk in chunk_list:\n",
    "        doc_path = '%s/%s.%s.gz' % (BASE_PROCESSED_PATH, chunk, lang_code)\n",
    "        url_doc_dict = filter_empty_docs(load_extracted(doc_path))\n",
    "        match_doc_dict = {}\n",
    "        for k, v in url_doc_dict.items():\n",
    "            if base_domain.strip().lower() in k.strip().lower():\n",
    "                match_doc_dict[k] = v\n",
    "        domain_doc_dict.update(match_doc_dict)\n",
    "    return domain_doc_dict\n",
    "\n",
    "\n",
    "def get_all_relevant_domain_data(chunk_list, base_domain, lang_code, embed_dict):\n",
    "    '''\n",
    "    return text_list_tokenized, embed_list, url list\n",
    "    '''\n",
    "    domain_doc_dict = load_all_chunks_for_domain(chunk_list, base_domain, lang_code) \n",
    "    obj_domain = map_dic2list(domain_doc_dict)\n",
    "    \n",
    "    text_list = obj_domain['text']\n",
    "    text_list_tokenized = [tokenize_doc_to_sentence(doc, lang_code) for doc in text_list]\n",
    "    \n",
    "    url_list = [url.strip() for url in obj_domain['mapping']]\n",
    "    embed_list = load_embeds_for_domain(embed_dict, lang_code,\n",
    "                                        text_list, url_list)\n",
    "    \n",
    "    return text_list_tokenized, embed_list, url_list\n",
    "\n",
    "\n",
    "def get_doc_embedding(url, text_list_tokenized,\n",
    "                      url_list,\n",
    "                      embedding_list,\n",
    "                      lidf_weighter,\n",
    "                      pca,\n",
    "                      pert_obj,\n",
    "                      doc_vec_method):\n",
    "    \n",
    "    i = url_list.index(url)\n",
    "    \n",
    "    return build_document_vector(text_list_tokenized[i],\n",
    "                        url_list[i],\n",
    "                        embedding_list[i],\n",
    "                        lidf_weighter,\n",
    "                        pca,\n",
    "                        pert_obj,\n",
    "                        doc_vec_method=doc_vec_method).doc_vector\n",
    "\n",
    "\n",
    "def handle_doc_embed_logic(embed_dict, src_url, tgt_url,\n",
    "                         src_lang_code, tgt_lang_code, doc_vector_method,\n",
    "                         pert_obj, \n",
    "                         lru_cache):\n",
    "    \n",
    "   \n",
    "    base_domain_src = regex_extractor_helper(BASE_DOMAIN_RE, src_url).strip()\n",
    "    base_domain_tgt = regex_extractor_helper(BASE_DOMAIN_RE, tgt_url).strip()\n",
    "    \n",
    "    #NOTE: NOT ALL Pairs share same base domain, ex: www.buyaas.com, si.buyaas.com, so url regex was adjusted     \n",
    "    if base_domain_src != base_domain_tgt:\n",
    "        print(base_domain_src, base_domain_tgt, \"DIFFERENT DOMAINS\")\n",
    "    assert base_domain_src == base_domain_tgt  \n",
    "    \n",
    "    \n",
    "    chunk_list_src = get_all_chunks_with_domain(embed_dict, base_domain_src) \n",
    "    chunk_list_tgt = get_all_chunks_with_domain(embed_dict, base_domain_src)\n",
    "    \n",
    "    cd = lru_cache.get(base_domain_src)\n",
    "    if cd == -1:\n",
    "        src_text_list_tokenized, src_embed_list, src_url_list = \\\n",
    "            get_all_relevant_domain_data(chunk_list_src, base_domain_src, src_lang_code, embed_dict)\n",
    "        \n",
    "        tgt_text_list_tokenized, tgt_embed_list, tgt_url_list = \\\n",
    "            get_all_relevant_domain_data(chunk_list_tgt, base_domain_tgt, tgt_lang_code, embed_dict)\n",
    "        cd = CachedData(src_text_list_tokenized, src_embed_list, src_url_list, \n",
    "                        tgt_text_list_tokenized, tgt_embed_list, tgt_url_list)\n",
    "        lru_cache.put(base_domain_src, cd)\n",
    "    else:\n",
    "        print(\"cache hit\")\n",
    "    src_text_list_tokenized, src_embed_list, src_url_list = cd.get_src_data()\n",
    "    tgt_text_list_tokenized, tgt_embed_list, tgt_url_list = cd.get_tgt_data()\n",
    "    pca, lidf_weighter = cd.get_fitted_objects()\n",
    "        \n",
    "\n",
    "    src_doc_embed = get_doc_embedding(src_url, src_text_list_tokenized, src_url_list, src_embed_list,\n",
    "                      lidf_weighter, pca, pert_obj, doc_vector_method)\n",
    "    tgt_doc_embed = get_doc_embedding(tgt_url, tgt_text_list_tokenized, tgt_url_list, tgt_embed_list,\n",
    "                      lidf_weighter, pca, pert_obj, doc_vector_method)\n",
    "    return src_doc_embed, tgt_doc_embed\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Datasets and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now defime dataloader class\n",
    "from threading import Lock\n",
    "\n",
    "\n",
    "#First get embed_dict, src_to_tgt_map, tgt_to_src_map\n",
    "#Then build pairset\n",
    "#Finally, at each idx, just get embed_src, embed_tgt, y\n",
    "class DocEmbedDataset(Dataset):\n",
    "    \n",
    "    \"\"\"\n",
    "    DocEmbeddingDataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, chunks_paths_list, src_lang, tgt_lang, doc_vector_method=\"SENT_ORDER\", cache_capacity=500): #NOTE: BE SURE TO ALLOCATE LOTS OF MEM\n",
    "      self.src_to_tgt_pairs, self.tgt_to_src_pairs = get_matching_url_dicts()\n",
    "      self.embed_dict = load_embed_dict(chunks_paths_list)\n",
    "      self.data_list = build_pair_dataset(self.embed_dict, self.src_to_tgt_pairs, self.tgt_to_src_pairs)\n",
    "      \n",
    "      self.src_lang = src_lang\n",
    "      self.tgt_lang = tgt_lang\n",
    "      \n",
    "      if doc_vector_method not in ['AVG', 'AVG_BP', 'SENT_ORDER']:\n",
    "        raise ValueError(\"\"\"Doc Vec Method must be one of the following:\n",
    "                            AVF, AVG_BP, SENT_ORDER\n",
    "                            Not found: %s\n",
    "                            \"\"\" % doc_vector_method)\n",
    "      self.doc_vector_method = doc_vector_method\n",
    "      self.pert_obj = ModifiedPertV2(None, None)\n",
    "      self.lru_cache = LRUCache(cache_capacity)\n",
    "      \n",
    "      #self.lock = Lock()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Get length of the dataset\n",
    "        \"\"\"\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self,\n",
    "                    idx):\n",
    "        \"\"\"\n",
    "        Gets the two vectors and target\n",
    "        \"\"\"\n",
    "        _, _, src_url, tgt_url, y_match_label = self.data_list[idx]\n",
    "        #print(\"CACHE LIST: %s \" %  list(self.lru_cache.keys())) #REMOVE AFTER DEBUG\n",
    "        #with self.lock:\n",
    "        #print(y_match_label, \"Y LABEL\", src_url, tgt_url, idx)\n",
    "        #TODO: TEMP, only to test training loop\n",
    "        src_doc_embedding, tgt_doc_embedding = load_embed_pairs(src_url, tgt_url, \n",
    "                                                                self.embed_dict,\n",
    "                                                                src_lang=self.src_lang,\n",
    "                                                                tgt_lang=self.tgt_lang)\n",
    "        src_doc_embedding, tgt_doc_embedding = src_doc_embedding[0], tgt_doc_embedding[0]\n",
    "        \n",
    "        '''                                               \n",
    "        src_doc_embedding, tgt_doc_embedding = handle_doc_embed_logic(self.embed_dict,\n",
    "                                                                      src_url, tgt_url,\n",
    "                                                                      self.src_lang, self.tgt_lang,\n",
    "                                                                      self.doc_vector_method,\n",
    "                                                                      self.pert_obj,\n",
    "                                                                      self.lru_cache) '''\n",
    "        return src_doc_embedding, tgt_doc_embedding, y_match_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function create_positive_samples took 0.02 seconds\n",
      "1306\n",
      "URL LIST LENGTHS 1306 2581 1306 1267\n",
      "LOOP COUNTER LEN 0\n",
      "2612\n",
      "2612\n",
      "Function create_positive_samples took 0.01 seconds\n",
      "1297\n",
      "URL LIST LENGTHS 1297 2537 1297 1231\n",
      "LOOP COUNTER LEN 0\n",
      "2594\n",
      "2594\n",
      "Function create_positive_samples took 0.01 seconds\n",
      "653\n",
      "URL LIST LENGTHS 653 1287 653 630\n",
      "LOOP COUNTER LEN 0\n",
      "1306\n",
      "1306\n"
     ]
    }
   ],
   "source": [
    "#Create dataloader\n",
    "train_dataset=DocEmbedDataset(chunks_paths_train[:2], SRC_LANG_CODE, TGT_LANG_CODE) #TODO: Remove after done debug of loop\n",
    "validation_dataset=DocEmbedDataset(chunks_paths_val[:2], SRC_LANG_CODE, TGT_LANG_CODE)\n",
    "test_dataset=DocEmbedDataset(chunks_paths_test[:1], SRC_LANG_CODE, TGT_LANG_CODE)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True) #prefetch_factor=5, prefetch maybe could help \n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=32, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2612 2594 1306\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'for i, vals in enumerate(train_dataloader):\\n    x_1, x_2, y = vals\\n    print(i, x_1.shape, x_2.shape, y, \"THIS BE PRINTING\")\\n    idx += 1\\n    if idx > 1:\\n        break'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 0\n",
    "print(len(train_dataset), len(validation_dataset), len(test_dataloader))\n",
    "#TODO: Continue to debug lack of embeds issue\n",
    "'''for i, vals in enumerate(train_dataloader):\n",
    "    x_1, x_2, y = vals\n",
    "    print(i, x_1.shape, x_2.shape, y, \"THIS BE PRINTING\")\n",
    "    idx += 1\n",
    "    if idx > 1:\n",
    "        break'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Plotting Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define loss func\n",
    "#Plot loss function\n",
    "def plot_loss_charts(train_loss_store, validation_loss_store):\n",
    "  '''\n",
    "  Plots loss charts over course of training\n",
    "  '''\n",
    "  ## Plotting epoch-wise test loss curve:\n",
    "  plt.plot(train_loss_store, '-o', label = 'train_loss', color = 'orange')\n",
    "  plt.plot(validation_loss_store, label = 'validation_loss', color = 'blue')\n",
    "  plt.xlabel('Epoch Number')\n",
    "  plt.ylabel('Loss At each epoch')\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "def plot_accuracy(train_score_store, validation_score_store, skip_plot=False):\n",
    "  '''\n",
    "  Plots Accuracy charts over course of training\n",
    "  '''\n",
    "  #Don't plot if this flag is set to true\n",
    "  if skip_plot:\n",
    "    return\n",
    "\n",
    "  ## Plotting epoch-wise test loss curve:\n",
    "  plt.plot(train_score_store, '-o', label = 'train_accuracy', color = 'orange')\n",
    "  plt.plot(validation_score_store, label = 'validation_accuracy', color = 'blue')\n",
    "  plt.xlabel('Epoch Number')\n",
    "  plt.ylabel('Accuracy At each epoch')\n",
    "  plt.legend()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocAlignerClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size_one=256, hidden_size_two=128, hidden_size_three=64):\n",
    "        super(DocAlignerClassifier, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size, hidden_size_one)\n",
    "        self.fc2 = nn.Linear(hidden_size_one, hidden_size_two)\n",
    "        self.fc3 = nn.Linear( hidden_size_two, hidden_size_three)\n",
    "        self.fc_out = nn.Linear(hidden_size_three, 1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        #Optional\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(64)\n",
    "                \n",
    "    def forward(self, src_doc_embed, tgt_doc_embed):\n",
    "        '''\n",
    "        Forward data through the lstm\n",
    "        '''\n",
    "        combined_input = torch.cat((src_doc_embed.view(src_doc_embed.size(0), -1),\n",
    "                          tgt_doc_embed.view(tgt_doc_embed.size(0), -1)), dim=1)\n",
    "        #print(combined_input.shape, \"combed input\")\n",
    "\n",
    "        x = self.relu(self.fc1(combined_input))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        output = self.fc_out(x)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Define Loss and Accuracy Eval Function\n",
    "def eval_acc_and_loss_func(model, loader, device, loss_metric, is_train = False, verbose = 1):\n",
    "    '''\n",
    "    Evaluate Function for CNN training\n",
    "    Slightly different than eval function from part 1\n",
    "    '''\n",
    "    correct, total, loss_sum = 0, 0, 0\n",
    "    \n",
    "    eval_type = \"Train\" if is_train else \"Validation\"\n",
    "    for X_1, X_2, Y in loader:\n",
    "        outputs, predicted, calculated_loss = None, None, None\n",
    "        X_1, X_2, Y = X_1.to(device), X_2.to(device), Y.to(device)\n",
    "\n",
    "        outputs = model(X_1, X_2)\n",
    "        \n",
    "        #Reshape output and turn y into a float\n",
    "        outputs = outputs.view(-1)\n",
    "        Y = Y.float()\n",
    "        predicted = torch.round(torch.sigmoid(outputs))\n",
    "        \n",
    "        total += Y.size(0)\n",
    "        \n",
    "        correct += (predicted == Y).sum().item()\n",
    "        calculated_loss = loss_metric(outputs,Y).item()\n",
    "        loss_sum += calculated_loss\n",
    "        \n",
    "    outputs, predicted, calculated_loss = None, None, None\n",
    "    if verbose:\n",
    "        print('%s accuracy: %f %%' % (eval_type, 100.0 * correct / total))\n",
    "        print('%s loss: %f' % (eval_type, loss_sum / total))\n",
    "    print\n",
    "    return 100.0 * correct / total, loss_sum/ total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINE TRAIN LOOP HERE\n",
    "def train(model,\n",
    "          optimizer,\n",
    "          loss_metric,\n",
    "          lr,\n",
    "          train_dataloader,\n",
    "          valid_dataloader,\n",
    "          device,\n",
    "          epochs=5,\n",
    "          stopping_threshold=3,\n",
    "          saving_per_epoch=10,\n",
    "          base_save_path=\"\",\n",
    "          model_name=\"\",\n",
    "          **kwargs):\n",
    "    \"\"\"\n",
    "    For each epoch, loop through batch,\n",
    "    compute forward and backward passes, apply gradient updates\n",
    "    Evaluate results and output\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    train_loss_store, train_acc_store = [], []\n",
    "    val_loss_store, val_acc_store, = [], []\n",
    "    start_epoch = 0\n",
    "\n",
    "    #Declare variables for early stopping\n",
    "    last_val_loss, stop_tracker = 100, 0\n",
    "\n",
    "    #training loop:\n",
    "    print(\"Starting Training\")\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "      time1 = time.time() #timekeeping\n",
    "      outputs, loss = None, None\n",
    "\n",
    "      model.train()\n",
    "      \n",
    "      correct_train, total, loss_sum = 0, 0, 0\n",
    "      for i, (x_1, x_2, y) in enumerate(train_dataloader):\n",
    "        \n",
    "        # Print device human readable names\n",
    "        #torch.cuda.get_device_name()\n",
    "\n",
    "        x_1, x_2, y = x_1.to(device), x_2.to(device), y.to(device)\n",
    "\n",
    "        #loss calculation and gradient update:\n",
    "\n",
    "        if i > 0 or epoch > 0:\n",
    "          optimizer.zero_grad()\n",
    "        outputs = model.forward(x_1, x_2)\n",
    "        \n",
    "        #Reshape output and turn y into a float\n",
    "        outputs = outputs.view(-1)\n",
    "        y = y.float()\n",
    "        #print(y.shape, outputs.shape, outputs, \"Loss inp info\")\n",
    "\n",
    "        \n",
    "        loss = loss_metric(outputs, y)\n",
    "        loss.backward()\n",
    "                      \n",
    "        ##performing update:\n",
    "        optimizer.step()\n",
    "\n",
    "        #Update Loss Info\n",
    "        loss_sum += loss.item()\n",
    "        predicted = torch.round(torch.sigmoid(outputs))\n",
    "        total += y.size(0)\n",
    "        correct_train += (predicted == y).sum().item()\n",
    "              \n",
    "      print(\"Epoch\",epoch+1,':')\n",
    "\n",
    "      \n",
    "      model.eval()\n",
    "      with torch.no_grad():\n",
    "\n",
    "        #Print Train Info\n",
    "        print('%s accuracy: %f %%' % (\"Train\", 100.0 * correct_train / total))\n",
    "        print('%s loss: %f' % (\"Train\", loss_sum / total))\n",
    "        print\n",
    "        \n",
    "        train_acc, train_loss = 100.0 * correct_train / total, loss_sum/ total\n",
    "        val_acc, val_loss = eval_acc_and_loss_func(model, valid_dataloader, device, loss_metric, is_train = False)\n",
    "\n",
    "        val_acc_store.append(val_acc)\n",
    "        val_loss_store.append(val_loss)\n",
    "\n",
    "        train_loss_store.append(train_loss)\n",
    "        train_acc_store.append(train_acc)\n",
    "\n",
    "      time2 = time.time() #timekeeping\n",
    "      #if show_progress:\n",
    "      print('Elapsed time for epoch:',time2 - time1,'s')\n",
    "      print('ETA of completion:',(time2 - time1)*(epochs - epoch - 1)/60,'minutes')\n",
    "      \n",
    "\n",
    "\n",
    "      #Handle early stopping logic\n",
    "      if val_loss >= last_val_loss:\n",
    "            stop_tracker += 1\n",
    "            if stop_tracker >= stopping_threshold:\n",
    "                print('Early Stopping triggered, Convergence has occured')\n",
    "                plot_loss_charts(train_loss_store, val_loss_store)\n",
    "                plot_accuracy(train_acc_store, val_acc_store)\n",
    "                #base_save_path, model_name, epoch + 1, model, optimizer)\n",
    "                #print(\"Model Copy Saved\")\n",
    "\n",
    "                return train_loss_store, val_acc_store\n",
    "      else:\n",
    "          stop_tracker = 0\n",
    "      last_val_loss = val_loss\n",
    "\n",
    "\n",
    "    plot_loss_charts(train_loss_store, val_loss_store)\n",
    "    plot_accuracy(train_acc_store, val_acc_store)\n",
    "    return train_loss_store, val_acc_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the model, and kick off training\n",
    "LEARNING_RATE = 0.001 #NOTE: CAN ADJUST\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = DocAlignerClassifier(2048) #Each doc vec is 2048, so times 2 will be 4096 TODO: Replace with 4096\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9)\n",
    "loss_criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training\n",
      "Epoch 1 :\n",
      "Train accuracy: 50.000000 %\n",
      "Train loss: 0.021812\n",
      "Validation accuracy: 50.000000 %\n",
      "Validation loss: 0.021949\n",
      "Elapsed time for epoch: 3069.637231826782 s\n",
      "ETA of completion: 204.6424821217855 minutes\n",
      "Epoch 2 :\n",
      "Train accuracy: 50.000000 %\n",
      "Train loss: 0.021789\n",
      "Validation accuracy: 50.000000 %\n",
      "Validation loss: 0.021930\n",
      "Elapsed time for epoch: 2985.1118845939636 s\n",
      "ETA of completion: 149.25559422969818 minutes\n",
      "Epoch 3 :\n",
      "Train accuracy: 50.000000 %\n",
      "Train loss: 0.021778\n",
      "Validation accuracy: 50.000000 %\n",
      "Validation loss: 0.021922\n",
      "Elapsed time for epoch: 2978.018518924713 s\n",
      "ETA of completion: 99.2672839641571 minutes\n",
      "Epoch 4 :\n",
      "Train accuracy: 50.000000 %\n",
      "Train loss: 0.021771\n",
      "Validation accuracy: 50.000000 %\n",
      "Validation loss: 0.021918\n",
      "Elapsed time for epoch: 2978.5933632850647 s\n",
      "ETA of completion: 49.64322272141774 minutes\n"
     ]
    }
   ],
   "source": [
    "#TODO: Test this with just base sent embeddings (as a place holder) just to see that training is working\n",
    "# Since its much faster than using doc vector\n",
    "train(\n",
    "        model,\n",
    "        optimizer,\n",
    "        loss_criterion,\n",
    "        LEARNING_RATE,\n",
    "        train_dataloader,\n",
    "        validation_dataloader,\n",
    "        device,\n",
    "        epochs=5\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
