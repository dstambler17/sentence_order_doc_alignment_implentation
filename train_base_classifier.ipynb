{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTHONPATH=\"/home/dstambl2/doc_alignment_implementations/thompson_2021_doc_align\"\n"
     ]
    }
   ],
   "source": [
    "%set_env PYTHONPATH=\"/home/dstambl2/doc_alignment_implementations/thompson_2021_doc_align\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Standard Library\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import defaultdict, namedtuple\n",
    "from random import shuffle, randint, sample, uniform, choice\n",
    "\n",
    "## External Libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as functional\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINE Training Hyperparams And Constants\n",
    "\n",
    "## Batch Size\n",
    "TRAIN_BATCH_SIZE = 32\n",
    "VAL_BATCH_SIZE = 32\n",
    "\n",
    "## Learning Rate\n",
    "LR = 0.001\n",
    "\n",
    "# Epochs (Consider setting high and implementing early stopping)\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "GPU_BOOL = torch.cuda.is_available()\n",
    "GPU_BOOL\n",
    "\n",
    "BASE_DATA_PATH=\"/home/dstambl2/doc_alignment_implementations/data\"\n",
    "BASE_EMBED_DIR = '/home/dstambl2/doc_alignment_implementations/data/cc_aligned_si_data/embeddings'\n",
    "\n",
    "BASE_PROCESSED_PATH=\"/home/dstambl2/doc_alignment_implementations/data/cc_aligned_si_data/processed\" \n",
    "ALIGNED_PAIRS_DOC = '/home/dstambl2/doc_alignment_implementations/data/cc_aligned_en_si.pairs'\n",
    "\n",
    "SRC_LANG_CODE=\"en\"\n",
    "TGT_LANG_CODE=\"si\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69 10 20\n"
     ]
    }
   ],
   "source": [
    "#Split into train, valid and test sets\n",
    "\n",
    "from utils.common import load_extracted, map_dic2list, \\\n",
    "    filter_empty_docs, regex_extractor_helper, tokenize_doc_to_sentence\n",
    "from modules.get_embeddings import read_in_embeddings, load_embeddings\n",
    "from modules.build_document_vector import build_document_vector\n",
    "from modules.vector_modules.boiler_plate_weighting import LIDFDownWeighting\n",
    "from align_docs import fit_pca_reducer\n",
    "from modules.vector_modules.window_func import ModifiedPertV2\n",
    "from utils.lru_cache import LRUCache\n",
    "from utils.common import function_timer\n",
    "\n",
    "CHUNK_RE = re.compile(r'(chunk_\\d*(?:_\\d*)?)', flags=re.IGNORECASE)\n",
    "BASE_DOMAIN_RE = re.compile(r'https?\\://(?:w{3}\\.)?(?:(?:si|en|sinhala|english)\\.)?(.*?)/', flags=re.IGNORECASE)\n",
    "\n",
    "#from sklearn.model_selection import train_test_split\n",
    "embed_chunk_paths = []\n",
    "for subdir, dirs, files in os.walk(BASE_EMBED_DIR):\n",
    "    if subdir != BASE_EMBED_DIR:\n",
    "        embed_chunk_paths.append(subdir)\n",
    "        \n",
    "embed_chunk_paths = sorted(embed_chunk_paths, key= lambda x: len(x))\n",
    "\n",
    "#For now split into 0.7/0.1/0.2 split\n",
    "#Split into train, test and val sets, remove last one for split since it will go into train set\n",
    "train_ind, test_ind = train_test_split(list(range(len(embed_chunk_paths[:-1]))), test_size=0.2, random_state=1)\n",
    "train_ind, val_ind = train_test_split(train_ind, test_size=0.125, random_state=1) # 0.125 x 0.8 = 0.1\n",
    "\n",
    "train_ind.append(len(embed_chunk_paths) -1) #Add last imbalenced idx to train set\n",
    "\n",
    "assert not any(set(train_ind) & set(test_ind)) and \\\n",
    "       not any(set(train_ind) & set(val_ind))and \\\n",
    "       not any(set(test_ind) & set(val_ind))\n",
    "\n",
    "print(len(train_ind), len(val_ind), len(test_ind))\n",
    "\n",
    "chunks_paths_train = [embed_chunk_paths[t] for t in train_ind]\n",
    "chunks_paths_val = [embed_chunk_paths[t] for t in val_ind]\n",
    "chunks_paths_test = [embed_chunk_paths[t] for t in test_ind]\n",
    "\n",
    "#print(chunks_paths_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not bool(set(chunks_paths_test) & set(chunks_paths_val))\n",
    "assert not bool(set(chunks_paths_train) & set(chunks_paths_val))\n",
    "assert not bool(set(chunks_paths_train) & set(chunks_paths_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list_reverseiterator' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/dstambl2/doc_alignment_implementations/thompson_2021_doc_align/train_base_classifier.ipynb Cell 6'\u001b[0m in \u001b[0;36m<cell line: 63>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blogin2.clsp.jhu.edu/home/dstambl2/doc_alignment_implementations/thompson_2021_doc_align/train_base_classifier.ipynb#ch0000005vscode-remote?line=59'>60</a>\u001b[0m         dom_counter[x] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blogin2.clsp.jhu.edu/home/dstambl2/doc_alignment_implementations/thompson_2021_doc_align/train_base_classifier.ipynb#ch0000005vscode-remote?line=60'>61</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mreversed\u001b[39m(\u001b[39msorted\u001b[39m([(k,v) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m dom_counter\u001b[39m.\u001b[39mitems()], key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: x[\u001b[39m1\u001b[39m]))[:\u001b[39m10\u001b[39m]\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Blogin2.clsp.jhu.edu/home/dstambl2/doc_alignment_implementations/thompson_2021_doc_align/train_base_classifier.ipynb#ch0000005vscode-remote?line=62'>63</a>\u001b[0m \u001b[39mprint\u001b[39m(filter_for_one_domain(load_embed_dict(chunks_paths_train), get_matching_url_dicts()[\u001b[39m0\u001b[39;49m] ))\n",
      "\u001b[1;32m/home/dstambl2/doc_alignment_implementations/thompson_2021_doc_align/train_base_classifier.ipynb Cell 6'\u001b[0m in \u001b[0;36mfilter_for_one_domain\u001b[0;34m(embed_dict, src_to_tgt)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blogin2.clsp.jhu.edu/home/dstambl2/doc_alignment_implementations/thompson_2021_doc_align/train_base_classifier.ipynb#ch0000005vscode-remote?line=58'>59</a>\u001b[0m     \u001b[39m#print(x)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blogin2.clsp.jhu.edu/home/dstambl2/doc_alignment_implementations/thompson_2021_doc_align/train_base_classifier.ipynb#ch0000005vscode-remote?line=59'>60</a>\u001b[0m     dom_counter[x] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Blogin2.clsp.jhu.edu/home/dstambl2/doc_alignment_implementations/thompson_2021_doc_align/train_base_classifier.ipynb#ch0000005vscode-remote?line=60'>61</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mreversed\u001b[39;49m(\u001b[39msorted\u001b[39;49m([(k,v) \u001b[39mfor\u001b[39;49;00m k, v \u001b[39min\u001b[39;49;00m dom_counter\u001b[39m.\u001b[39;49mitems()], key\u001b[39m=\u001b[39;49m\u001b[39mlambda\u001b[39;49;00m x: x[\u001b[39m1\u001b[39;49m]))[:\u001b[39m10\u001b[39;49m]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'list_reverseiterator' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "\n",
    "''' The following two funcs are only for getting sent embeds '''\n",
    "def get_base_embedding(url, embedding_file_path, lang_code):\n",
    "\n",
    "    chunk = regex_extractor_helper(CHUNK_RE, embedding_file_path)\n",
    "    doc_path = '%s/%s.%s.gz' % (BASE_PROCESSED_PATH, chunk, lang_code)\n",
    "    url_doc_dict = filter_empty_docs(load_extracted(doc_path))\n",
    "    if url not in url_doc_dict:\n",
    "        print(\"missing url in get_base_embedding %s for doc path %s\" % (url, doc_path))\n",
    "        #return noise\n",
    "        return np.random.random((1,1024))\n",
    "    doc_text = url_doc_dict[url]\n",
    "        \n",
    "    _, sent_embeds = read_in_embeddings(doc_text, embedding_file_path, lang_code)\n",
    "    \n",
    "    return sent_embeds\n",
    "\n",
    "\n",
    "def load_embed_pairs(src_url, tgt_url, embed_dict,\n",
    "                    src_lang=SRC_LANG_CODE,\n",
    "                    tgt_lang=TGT_LANG_CODE):\n",
    "    src_path = embed_dict[src_url]\n",
    "    tgt_path = embed_dict[tgt_url]\n",
    "    line_embeddings_src = get_base_embedding(src_url, src_path, src_lang)\n",
    "    line_embeddings_tgt = get_base_embedding(tgt_url, tgt_path, tgt_lang)\n",
    "    \n",
    "    #print(line_embeddings_src.shape, line_embeddings_tgt.shape)\n",
    "    return line_embeddings_src, line_embeddings_tgt\n",
    "''' END OF PURE SENT EMBED FUNCS '''\n",
    "\n",
    "def get_matching_url_dicts(input_path = ALIGNED_PAIRS_DOC):\n",
    "    src_to_tgt = {}\n",
    "    tgt_to_src = {}\n",
    "    with open(input_path, 'r') as fp:\n",
    "        for row in fp:\n",
    "            src, tgt = row.split('\\t')\n",
    "            src_to_tgt[src] = tgt\n",
    "            tgt_to_src[tgt]  = src\n",
    "    return src_to_tgt, tgt_to_src\n",
    "\n",
    "\n",
    "def load_embed_dict(chunks_paths):\n",
    "    embed_dict = {}\n",
    "    \n",
    "    for chunk_path in chunks_paths:\n",
    "        embed_dict_path = '%s/embedding_lookup.json' % (chunk_path)\n",
    "        embed_dict_chunk = {}\n",
    "        with open(embed_dict_path, 'r') as f:\n",
    "            embed_dict_chunk = json.load(f)\n",
    "        embed_dict.update(embed_dict_chunk)\n",
    "                \n",
    "    return embed_dict\n",
    "\n",
    "'''Helper function for finding the most prominent domains to see inner doc sim'''\n",
    "def filter_for_one_domain(embed_dict, src_to_tgt):\n",
    "    dom_counter = defaultdict(int)\n",
    "    \n",
    "    for k, _ in src_to_tgt.items():\n",
    "        x = BASE_DOMAIN_RE.findall(k)[0]\n",
    "        #print(x)\n",
    "        dom_counter[x] += 1\n",
    "    return list(reversed(sorted([(k,v) for k, v in dom_counter.items()], key=lambda x: x[1])))[:10]\n",
    "\n",
    "print(filter_for_one_domain(load_embed_dict(chunks_paths_train), get_matching_url_dicts()[0] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Sample Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CandidateTuple = namedtuple(\n",
    "    \"CandidateTuple\", \"src_embed_path, tgt_embed_path, src_url, tgt_url, y_match_label, augment_neg_data_method\")\n",
    "\n",
    "\n",
    "''' Sampling logic start'''\n",
    "@function_timer\n",
    "def create_positive_samples(embed_dict, src_to_tgt_map, tgt_to_src_map, data_list):\n",
    "    '''\n",
    "    Builds positive samples\n",
    "    '''\n",
    "    for src_url, tgt_url in src_to_tgt_map.items():\n",
    "        src_url, tgt_url = src_url.strip(), tgt_url.strip()\n",
    "        if src_url in embed_dict and tgt_url in embed_dict:\n",
    "            src_embed_path, tgt_embed_path = embed_dict[src_url], embed_dict[tgt_url]\n",
    "            c = CandidateTuple(src_embed_path, tgt_embed_path, src_url, tgt_url, 1, None)\n",
    "            data_list.append(c)\n",
    "\n",
    "\n",
    "def create_all_possible_neg_pairs(src_to_tgt_map, tgt_to_src_map):\n",
    "    \n",
    "    src_url_list, tgt_url_list = list(src_to_tgt_map.keys()), list(tgt_to_src_map.keys())\n",
    "    domain_dict = defaultdict(lambda: defaultdict(list)) #Ex{dom: {src: [], tgt: []}}\n",
    "    for url in tgt_url_list:\n",
    "        #url = url.strip()\n",
    "        base_url = regex_extractor_helper(BASE_DOMAIN_RE, url).strip()\n",
    "        domain_dict[base_url]['tgt'].append(url)\n",
    "    \n",
    "    for url in src_url_list:\n",
    "        #url = url.strip()\n",
    "        base_url = regex_extractor_helper(BASE_DOMAIN_RE, url).strip()\n",
    "        domain_dict[base_url]['src'].append(url)\n",
    "    \n",
    "    negative_sample_dict = {}\n",
    "    #Loop through all domains and create final negative pairing\n",
    "    for domain, values in domain_dict.items():\n",
    "        sample_list = []\n",
    "        src_urls, tgt_urls = values['src'], values['tgt']\n",
    "        \n",
    "        if len(src_urls) > 100:\n",
    "            shuffle(src_urls)\n",
    "        if len(tgt_urls) > 100:\n",
    "            shuffle(tgt_urls)\n",
    "        for src_url in src_urls[:min(100, len(src_urls))]:\n",
    "            for tgt_url in tgt_urls[:min(100, len(src_urls))]:\n",
    "                if src_to_tgt_map[src_url] != tgt_url and tgt_to_src_map[tgt_url] != src_url:\n",
    "                    sample_list.append((src_url, tgt_url))\n",
    "        if len(sample_list) > 0:\n",
    "            negative_sample_dict[domain] = sample_list\n",
    "    return negative_sample_dict\n",
    "        \n",
    "    \n",
    "\n",
    "def same_domain_neg_sample_helper(negative_sample_dict):\n",
    "    '''\n",
    "    IDEA: randomly modify docs  \n",
    "    Helper function for returning\n",
    "    Negative domains of same idx\n",
    "\n",
    "    Algo: 1) Pick random domain\n",
    "         2) Pick random sample from that domain\n",
    "         3) pop that sample\n",
    "    '''\n",
    "    domain_list = list(negative_sample_dict.keys())\n",
    "    domain = domain_list[randint(0, len(domain_list) - 1)]\n",
    "    \n",
    "    neg_pair_list = negative_sample_dict[domain]\n",
    "    neg_pair_idx = randint(0, len(neg_pair_list) - 1)\n",
    "    src_url, tgt_url = neg_pair_list[neg_pair_idx]\n",
    "    \n",
    "    #Update the dict to remove neg pair\n",
    "    neg_pair_list.pop(neg_pair_idx)\n",
    "    if len(neg_pair_list) == 0:\n",
    "        negative_sample_dict.pop(domain)\n",
    "    else:\n",
    "        negative_sample_dict[domain] = neg_pair_list\n",
    "\n",
    "    return src_url, tgt_url\n",
    "\n",
    "\n",
    "def create_negative_samples_diff_docs(embed_dict, src_to_tgt_map, tgt_to_src_map, data_list, precent_cutoff):\n",
    "    '''\n",
    "    Builds negative samples\n",
    "    '''\n",
    "\n",
    "    number_pos_samples = len(data_list)\n",
    "    visited_urls, MAX_INSTANCES_ALLOWED = defaultdict(int), 10 \n",
    "    \n",
    "    negative_sample_dict = create_all_possible_neg_pairs(src_to_tgt_map, tgt_to_src_map)    \n",
    "    \n",
    "    \n",
    "    print(\"URL LIST LENGTHS\", number_pos_samples, len(embed_dict), len(src_to_tgt_map), len(tgt_to_src_map))\n",
    "    num_samples = int(number_pos_samples * precent_cutoff)\n",
    "    \n",
    "    loop_counter = 0 #NOTE: FOR DEBUG\n",
    "    for i in range(num_samples):\n",
    "        \n",
    "        while True:\n",
    "            src_url, tgt_url = same_domain_neg_sample_helper(negative_sample_dict)\n",
    "\n",
    "            #Repeat until all conditions are met\n",
    "            if (visited_urls[src_url.strip()] < MAX_INSTANCES_ALLOWED and \\\n",
    "                visited_urls[tgt_url.strip()] < MAX_INSTANCES_ALLOWED and \\\n",
    "                src_to_tgt_map[src_url] != tgt_url and tgt_to_src_map[tgt_url] != src_url):\n",
    "                \n",
    "                src_url, tgt_url = src_url.strip(), tgt_url.strip()\n",
    "                if src_url in embed_dict and tgt_url in embed_dict:\n",
    "                    src_embed_path, tgt_embed_path = embed_dict[src_url], embed_dict[tgt_url]\n",
    "                    c = CandidateTuple(src_embed_path, tgt_embed_path, src_url, tgt_url, 0, None)\n",
    "                    data_list.append(c)\n",
    "                \n",
    "                    visited_urls[src_url] += 1\n",
    "                    visited_urls[tgt_url] += 1\n",
    "                    \n",
    "                break\n",
    "            else:\n",
    "                loop_counter += 1\n",
    "\n",
    "    print(\"LOOP COUNTER LEN %s\" % loop_counter)\n",
    "    print(len(data_list))\n",
    "\n",
    "def create_negative_samples_from_aligned_pairs(embed_dict, src_to_tgt_map, tgt_to_src_map, data_list, precent_cutoff):\n",
    "    '''\n",
    "    From positive samples, create negative ones, to increase sensitivity of classifier\n",
    "    By dropping random handful of sents\n",
    "    In general, the idx method will deal with sent dropping/ adding\n",
    "    '''\n",
    "    pos_list = [(src_url, tgt_url) for src_url, tgt_url in src_to_tgt_map.items()]\n",
    "    \n",
    "    sample_size = int(len(pos_list) * precent_cutoff)\n",
    "    pos_list = sample(pos_list, sample_size)\n",
    "\n",
    "    #Sample half of list and alternate which methods should be used\n",
    "    method_idx, methods = 0, [\"add\", \"delete\", \"sub\"]\n",
    "    \n",
    "    for src_url, tgt_url in pos_list:\n",
    "        src_url, tgt_url = src_url.strip(), tgt_url.strip()\n",
    "        if src_url in embed_dict and tgt_url in embed_dict:\n",
    "            src_embed_path, tgt_embed_path = embed_dict[src_url], embed_dict[tgt_url]\n",
    "            c = CandidateTuple(src_embed_path, tgt_embed_path, src_url, tgt_url, 0, methods[method_idx]) \n",
    "            method_idx  = (method_idx + 1) % len(methods)\n",
    "            data_list.append(c)\n",
    "\n",
    "        \n",
    "#TODO: Uncomment when done with debug\n",
    "def build_pair_dataset(embed_dict, src_to_tgt_map, tgt_to_src_map):\n",
    "    '''\n",
    "    Create positive and negative url tuple pairs\n",
    "    Makeup of dataset will be:\n",
    "        50% pure positive pairs\n",
    "        25% pure negative pairs\n",
    "        25% negative pairs that are close to each other\n",
    "    '''\n",
    "    data_list = []\n",
    "    create_positive_samples(embed_dict, src_to_tgt_map, tgt_to_src_map, data_list) #First Create positive samples\n",
    "    print(len(data_list))\n",
    "    \n",
    "    subset_src_to_tgt_map = {k: v for k,v in src_to_tgt_map.items() if k.strip() in embed_dict and v.strip() in embed_dict}\n",
    "    subset_tgt_to_src_map = {k: v for k,v in tgt_to_src_map.items() if k.strip() in embed_dict and v.strip() in embed_dict}\n",
    "    \n",
    "    diff_neg_precent, close_neg_percent = 0.5, 0.5\n",
    "    create_negative_samples_diff_docs(embed_dict, subset_src_to_tgt_map, subset_tgt_to_src_map, data_list, diff_neg_precent)  #Now create negative samples\n",
    "    create_negative_samples_from_aligned_pairs(embed_dict, subset_src_to_tgt_map, subset_tgt_to_src_map, data_list, close_neg_percent)  #create negative samples by modding positive ones\n",
    "    #Shuffle and return data\n",
    "    shuffle(data_list)\n",
    "    print(len(data_list))\n",
    "\n",
    "    return data_list\n",
    "\n",
    "en_to_si_pairs, si_to_en_pairs = get_matching_url_dicts()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function create_positive_samples took 0.25 seconds\n",
      "44385\n",
      "URL LIST LENGTHS 44385 87552 44385 43034\n",
      "LOOP COUNTER LEN 14\n",
      "66577\n",
      "88769\n",
      "https://www.jsjlmachinery.com/about-us/contact-us/ http://centers.cultural.gov.lk/batticaloa/index.php?option=com_content&view=frontpage&Itemid=65&lid=mm&mid=1&lang=en\n"
     ]
    }
   ],
   "source": [
    "#TEST ABOVE CODE\n",
    "embed_dict_train = load_embed_dict(chunks_paths_train)\n",
    "data_list = build_pair_dataset(embed_dict_train, en_to_si_pairs, si_to_en_pairs)\n",
    "sample_one = list(embed_dict_train.keys())[10]\n",
    "sample_two = list(embed_dict_train.keys())[57]\n",
    "print(sample_one, sample_two)\n",
    "#load_embed_pairs(sample_one, sample_two, embed_dict_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper logic (PCA/Cache) for Doc Embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn.decomposition import PCA\n",
    "def fit_pca_reducer_debug(embedding_list_src, embedding_list_tgt):\n",
    "    '''\n",
    "    Builds PCA Dim Reduction from sample of sentence embeddings\n",
    "    in the webdomain\n",
    "    '''\n",
    "    all_sent_embeds = np.vstack(embedding_list_src + embedding_list_tgt)\n",
    "\n",
    "    pca = PCA(n_components=128)\n",
    "    divide_num = 1\n",
    "    if len(all_sent_embeds) // 6 >= 128:\n",
    "        divide_num = 6\n",
    "    elif len(all_sent_embeds) // 5 >= 128:\n",
    "        divide_num = 5\n",
    "    elif len(all_sent_embeds) // 4 >= 128:\n",
    "        divide_num = 4\n",
    "    elif len(all_sent_embeds) // 3 >= 128:\n",
    "        divide_num = 3\n",
    "    elif len(all_sent_embeds) // 2 >= 128:\n",
    "        divide_num = 2\n",
    "    elif len(all_sent_embeds) // 1 >= 128:\n",
    "        divide_num = 1\n",
    "    else:\n",
    "        sent_size = all_sent_embeds.shape[0]\n",
    "        num_iters = int(math.ceil(128 / sent_size))        \n",
    "        all_sent_embeds = np.repeat(all_sent_embeds, repeats=num_iters, axis=0)\n",
    "        \n",
    "\n",
    "    my_rand_int = np.random.randint(all_sent_embeds.shape[0], size=len(all_sent_embeds) // divide_num)\n",
    "    pca_fit_data = all_sent_embeds[my_rand_int, :]\n",
    "    pca.fit(pca_fit_data)\n",
    "    return pca\n",
    "\n",
    "#DEFINE Helper functions for building doc vectors\n",
    "#NOTE: Much of this info will be stored in an LRU cache\n",
    "\n",
    "\n",
    "class CachedData:\n",
    "    '''\n",
    "    Keeps organized cache of data\n",
    "    Keys will be domain_name\n",
    "    Since for each domain, we want the source and target lang info\n",
    "    '''\n",
    "    def __init__(self, src_text_list_tokenized,\n",
    "                       src_embed_list,\n",
    "                       src_url_list,\n",
    "                       tgt_text_list_tokenized,\n",
    "                       tgt_embed_list,\n",
    "                       tgt_url_list,\n",
    "                       ):\n",
    "        self.src_text_list_tokenized = src_text_list_tokenized\n",
    "        self.src_embed_list = src_embed_list\n",
    "        self.src_url_list = src_url_list\n",
    "\n",
    "        self.tgt_text_list_tokenized = tgt_text_list_tokenized\n",
    "        self.tgt_embed_list = tgt_embed_list\n",
    "        self.tgt_url_list = tgt_url_list\n",
    "        \n",
    "        self.lidf_weighter = LIDFDownWeighting(src_text_list_tokenized + tgt_text_list_tokenized)\n",
    "        self.pca = fit_pca_reducer_debug(src_embed_list, tgt_embed_list)\n",
    "    \n",
    "    def get_fitted_objects(self):\n",
    "        '''\n",
    "        Return PCA and LIDF\n",
    "        '''\n",
    "        return self.pca, self.lidf_weighter\n",
    "\n",
    "    def get_src_data(self):\n",
    "        return self.src_text_list_tokenized, self.src_embed_list, self.src_url_list\n",
    "    \n",
    "    def get_tgt_data(self):\n",
    "        return self.tgt_text_list_tokenized, self.tgt_embed_list, self.tgt_url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_augment_helper(src_text_list_tokenized, tgt_text_list_tokenized, augment_negative_data_method):\n",
    "    '''\n",
    "    Goal is to pick which to augment (src or tgt) and to confirm that adding/subing is possible\n",
    "    This logic is here, because when we add or sub, we are assuming that other docs exist\n",
    "    1) Randomly pick if augment_negative_data_method is delete\n",
    "    2) If add/sub, pick rand or larger of two\n",
    "    '''\n",
    "    rv = uniform(0, 1)\n",
    "    random_change_item = \"src\" if rv >= 0.5 else \"tgt\"\n",
    "    \n",
    "    if augment_negative_data_method in [\"add\", \"sub\"]:\n",
    "        if len(tgt_text_list_tokenized) == 1 and len(src_text_list_tokenized) == 1:\n",
    "            augment_negative_data_method = \"delete\"\n",
    "            item_to_change = random_change_item\n",
    "        elif len(tgt_text_list_tokenized) == 1 and len(src_text_list_tokenized) > 1:\n",
    "            item_to_change = \"src\"\n",
    "        elif len(tgt_text_list_tokenized) > 1 and len(src_text_list_tokenized) == 1:\n",
    "            item_to_change = \"tgt\"\n",
    "        else:\n",
    "            item_to_change = random_change_item\n",
    "    \n",
    "    else:\n",
    "        item_to_change = random_change_item\n",
    "    \n",
    "    return augment_negative_data_method, item_to_change\n",
    "\n",
    "\n",
    "#Cell for dealing with augmentation\n",
    "def create_augmented_negative_sample(src_url, src_text_list_tokenized, src_url_list, src_embed_list,\n",
    "                                    tgt_url, tgt_text_list_tokenized, tgt_url_list, tgt_embed_list,\n",
    "                                    augment_negative_data_method\n",
    "                                    ):\n",
    "    '''\n",
    "    Builds negative sample from either\n",
    "    1 dropping a few sentences\n",
    "    2. Substituting sents from another doc\n",
    "    3. Adding sentences from other docs\n",
    "    \n",
    "    Algo\n",
    "    1) Randomly pick if modifying source of target first\n",
    "        If add/sub, pick domains with more data\n",
    "    2) If both src and tgt have only 1 doc, default to delete method\n",
    "    3) For deletion, pick random num of rand indexes of modified item\n",
    "    4) For sub, delete a random index and sub out rand sents with another doc\n",
    "    5) For add, just tack on rand sents\n",
    "\n",
    "    '''\n",
    "    METHODS = [\"add\", \"delete\", \"sub\"]\n",
    "    if augment_negative_data_method not in METHODS:\n",
    "        print(\"method not in list\")\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def helper_delete_multiple_element(list_object, indices):\n",
    "        '''\n",
    "        Helper function taken from\n",
    "        https://thispointer.com/python-remove-elements-from-list-by-index/\n",
    "        '''\n",
    "        indices = sorted(indices, reverse=True)\n",
    "        for idx in indices:\n",
    "            if idx < len(list_object):\n",
    "                list_object.pop(idx)\n",
    "    \n",
    "    def delete_logic(url, change_text_list_tokenized, change_url_list, change_embed_list):\n",
    "        '''\n",
    "        Handles deletion logic\n",
    "        '''\n",
    "        #First get data that will be augmented\n",
    "        i = change_url_list.index(url)\n",
    "        change_doc = change_text_list_tokenized[i]\n",
    "        \n",
    "        num_sents = change_embed_list[i].shape[0]\n",
    "        \n",
    "        #Note call add logic in this case\n",
    "        if num_sents == 1:\n",
    "            add_logic(url, change_text_list_tokenized, change_url_list, change_embed_list)\n",
    "            return\n",
    "        change_sent_embeds = change_embed_list[i].tolist()\n",
    "        \n",
    "        #Randomly pick how many and which sents to drop, then drop them\n",
    "        num_sents_to_drop = randint(1, max(int(math.ceil((len(change_doc) - 1) / 5)), 1))\n",
    "        \n",
    "        #Make sure that there is always going to be one sentence left\n",
    "        num_sents_to_drop = min(num_sents_to_drop, num_sents - 1)\n",
    "        \n",
    "        index_values = sample(list(enumerate(change_doc)), num_sents_to_drop)\n",
    "        drop_index_vals = [x[0] for x in index_values]\n",
    "        \n",
    "        helper_delete_multiple_element(change_doc, drop_index_vals)\n",
    "        helper_delete_multiple_element(change_sent_embeds, drop_index_vals)\n",
    "        \n",
    "        assert len(change_doc) == len(change_sent_embeds)\n",
    "        \n",
    "        #Update the altered doc \n",
    "        change_text_list_tokenized[i] = change_doc\n",
    "        change_embed_list[i] = np.asarray(change_sent_embeds)\n",
    "        \n",
    "    def sub_logic(url, change_text_list_tokenized, change_url_list, change_embed_list):\n",
    "        '''\n",
    "        Handles substitution logic\n",
    "        '''\n",
    "        #First get data that will be augmented\n",
    "        i = change_url_list.index(url)\n",
    "        change_doc = change_text_list_tokenized[i]\n",
    "        change_sent_embeds = change_embed_list[i]\n",
    "        \n",
    "        #Randomly pick how many sents to sub\n",
    "        num_sents_to_sub = randint(1, max(int(math.ceil((len(change_doc) - 1) / 3)), 2))\n",
    "        #Randomly pick which doc\n",
    "        sub_idx = choice([j for j in range(len(change_text_list_tokenized)) if j!=i])\n",
    "        \n",
    "        #Randomly pick sents to sub with\n",
    "        sub_doc, sub_embed_matrix = change_text_list_tokenized[sub_idx], change_embed_list[sub_idx]\n",
    "        \n",
    "        cut_off_point = min(len(change_doc), len(sub_doc))\n",
    "        num_sents_to_sub = min(num_sents_to_sub, len(sub_doc[:cut_off_point]))\n",
    "        index_values = sample(list(enumerate(sub_doc[:cut_off_point])), num_sents_to_sub)\n",
    "        sub_index_vals = [x[0] for x in index_values]\n",
    "        \n",
    "        for idx in sub_index_vals:\n",
    "            change_doc[idx] = sub_doc[idx]\n",
    "            change_sent_embeds[idx] = sub_embed_matrix[idx]\n",
    "        \n",
    "        assert len(change_doc) == len(change_sent_embeds)\n",
    "        #Update the altered doc\n",
    "        change_text_list_tokenized[i] = change_doc\n",
    "        change_embed_list[i] = change_sent_embeds\n",
    "\n",
    "    \n",
    "    def add_logic(url, change_text_list_tokenized, change_url_list, change_embed_list):\n",
    "        '''\n",
    "        Handles addition logic\n",
    "        '''\n",
    "        #First get data that will be augmented\n",
    "        i = change_url_list.index(url)\n",
    "        change_doc = change_text_list_tokenized[i]\n",
    "        change_sent_embeds = change_embed_list[i]\n",
    "        \n",
    "        #Randomly pick how many sents to adds\n",
    "        num_sents_to_add = randint(1, max(int(math.ceil((len(change_doc) - 1) / 5)), 1))\n",
    "        #Randomly pick which doc\n",
    "        add_idx = choice([j for j in range(len(change_text_list_tokenized)) if j!=i])\n",
    "        \n",
    "        #Randomly pick sents to add\n",
    "        add_doc, add_embed_matrix = change_text_list_tokenized[add_idx], change_embed_list[add_idx]\n",
    "        \n",
    "        index_values = sample(list(enumerate(add_doc)), num_sents_to_add)\n",
    "        add_index_vals = [x[0] for x in index_values]\n",
    "        add_doc_sents = [add_doc[idx] for idx in add_index_vals]\n",
    "        add_embed_sents = [add_embed_matrix[idx] for idx in add_index_vals]\n",
    "        \n",
    "        #Add sents\n",
    "        change_doc += add_doc_sents\n",
    "        change_sent_embeds = np.append(change_sent_embeds, np.asarray(add_embed_sents), axis=0)\n",
    "        \n",
    "        assert len(change_doc) == len(change_sent_embeds)\n",
    "        \n",
    "        #Update the altered doc\n",
    "        change_text_list_tokenized[i] = change_doc\n",
    "        change_embed_list[i] = change_sent_embeds\n",
    "    \n",
    "    \n",
    "    augment_negative_data_method, item_to_change = create_augment_helper(src_text_list_tokenized,\n",
    "                                                                         tgt_text_list_tokenized,\n",
    "                                                                         augment_negative_data_method)\n",
    "    if item_to_change == \"src\":\n",
    "        url, change_text_list_tokenized, change_url_list, change_embed_list = src_url, src_text_list_tokenized, src_url_list, src_embed_list\n",
    "    else:\n",
    "        url, change_text_list_tokenized, change_url_list, change_embed_list = tgt_url, tgt_text_list_tokenized, tgt_url_list, tgt_embed_list\n",
    "    \n",
    "    if augment_negative_data_method == \"add\":\n",
    "        #Add from one doc to another\n",
    "        add_logic(url, change_text_list_tokenized, change_url_list, change_embed_list)\n",
    "    elif augment_negative_data_method == \"delete\":\n",
    "        delete_logic(url, change_text_list_tokenized, change_url_list, change_embed_list)\n",
    "        #delete random number of sentences from source and target\n",
    "    elif augment_negative_data_method == \"sub\":\n",
    "        sub_logic(url, change_text_list_tokenized, change_url_list, change_embed_list)\n",
    "        #swap sentences \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain Specific Doc Embedding Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_embeds_for_domain(embed_dict, lang_code, text_list, url_list):\n",
    "    '''\n",
    "    Load in embeds for a domain\n",
    "    ''' \n",
    "    embed_list = []\n",
    "    try:\n",
    "        for ii in range(len(text_list)):\n",
    "            url, doc_text = url_list[ii], text_list[ii]\n",
    "            #if url in embed_dict: #TEMP FIX: TODO: Rerun embeds and issue of missing URLS here or delete sentences that miss embeds\n",
    "            embed_file_path = embed_dict[url]\n",
    "            try:\n",
    "                _, embeddings = read_in_embeddings(doc_text, embed_file_path, lang_code)\n",
    "                embed_list.append(embeddings)\n",
    "            except Exception as e:\n",
    "                print(embed_file_path, lang_code, \"EMBED EXCEPTION\", e)\n",
    "\n",
    "    except KeyError as e: #For debugging\n",
    "        print(e)\n",
    "        print(\"EXCEPTION OCCURED in load_embeds_for_domain\")\n",
    "    \n",
    "    return embed_list\n",
    "\n",
    "\n",
    "''' Domain Specific Chunk helper data'''\n",
    "\n",
    "def get_all_chunks_with_domain(embed_dict, base_domain):\n",
    "    '''\n",
    "    First get a list of all chunks that contain the domain_name\n",
    "    Second get all docs in src lang and tgt lang\n",
    "    Third, get pca, ldf weighter and more\n",
    "    '''\n",
    "    chunks = [regex_extractor_helper(CHUNK_RE, value) \\\n",
    "                for key, value in embed_dict.items() if base_domain.strip() in key.strip().lower()]\n",
    "    if len(chunks) == 0:\n",
    "        print(\"WAIT, CHUNK LIST IS EMPTY, so CHUNK_RE error\")\n",
    "    return list(set(chunks))\n",
    "\n",
    "def load_all_chunks_for_domain(chunk_list, base_domain, lang_code):\n",
    "    '''\n",
    "    Given a list of domain chunks and the domain_name\n",
    "    Get a domain doc dict\n",
    "    '''\n",
    "    domain_doc_dict = {}\n",
    "    for chunk in chunk_list:\n",
    "        doc_path = '%s/%s.%s.gz' % (BASE_PROCESSED_PATH, chunk, lang_code)\n",
    "        url_doc_dict = filter_empty_docs(load_extracted(doc_path))\n",
    "        match_doc_dict = {}\n",
    "        for k, v in url_doc_dict.items():\n",
    "            if base_domain.strip().lower() in k.strip().lower():\n",
    "                match_doc_dict[k] = v\n",
    "        domain_doc_dict.update(match_doc_dict)\n",
    "    return domain_doc_dict\n",
    "\n",
    "\n",
    "def get_all_relevant_domain_data(chunk_list, base_domain, lang_code, embed_dict):\n",
    "    '''\n",
    "    return text_list_tokenized, embed_list, url list\n",
    "    '''\n",
    "    domain_doc_dict = load_all_chunks_for_domain(chunk_list, base_domain, lang_code) \n",
    "    obj_domain = map_dic2list(domain_doc_dict)\n",
    "    \n",
    "    text_list = obj_domain['text']\n",
    "    text_list_tokenized = [tokenize_doc_to_sentence(doc, lang_code) for doc in text_list]\n",
    "    \n",
    "    url_list = [url.strip() for url in obj_domain['mapping']]\n",
    "    embed_list = load_embeds_for_domain(embed_dict, lang_code,\n",
    "                                        text_list, url_list)\n",
    "    \n",
    "    return text_list_tokenized, embed_list, url_list\n",
    "\n",
    "\n",
    "\n",
    "def get_doc_embedding(url, text_list_tokenized,\n",
    "                      url_list,\n",
    "                      embedding_list,\n",
    "                      lidf_weighter,\n",
    "                      pca,\n",
    "                      pert_obj,\n",
    "                      doc_vec_method):\n",
    "    '''\n",
    "    Call doc embedding method\n",
    "    '''\n",
    "    i = url_list.index(url)\n",
    "    \n",
    "    return build_document_vector(text_list_tokenized[i],\n",
    "                        url_list[i],\n",
    "                        embedding_list[i],\n",
    "                        lidf_weighter,\n",
    "                        pca,\n",
    "                        pert_obj,\n",
    "                        doc_vec_method=doc_vec_method).doc_vector\n",
    "\n",
    "\n",
    "def handle_doc_embed_logic(embed_dict, src_url, tgt_url,\n",
    "                         src_lang_code, tgt_lang_code, doc_vector_method,\n",
    "                         pert_obj, \n",
    "                         lru_cache,\n",
    "                         augment_negative_data_method):\n",
    "    \n",
    "   \n",
    "    base_domain_src = regex_extractor_helper(BASE_DOMAIN_RE, src_url).strip()\n",
    "    base_domain_tgt = regex_extractor_helper(BASE_DOMAIN_RE, tgt_url).strip()\n",
    "    \n",
    "    #NOTE: NOT ALL Pairs share same base domain, ex: www.buyaas.com, si.buyaas.com, so url regex was adjusted     \n",
    "    if base_domain_src != base_domain_tgt:\n",
    "        print(base_domain_src, base_domain_tgt, \"DIFFERENT DOMAINS\")\n",
    "    assert base_domain_src == base_domain_tgt  \n",
    "    \n",
    "    \n",
    "    chunk_list_src = get_all_chunks_with_domain(embed_dict, base_domain_src) \n",
    "    chunk_list_tgt = get_all_chunks_with_domain(embed_dict, base_domain_src)\n",
    "    \n",
    "    cd = lru_cache.get(base_domain_src)\n",
    "    if cd == -1:\n",
    "        src_text_list_tokenized, src_embed_list, src_url_list = \\\n",
    "            get_all_relevant_domain_data(chunk_list_src, base_domain_src, src_lang_code, embed_dict)\n",
    "        \n",
    "        tgt_text_list_tokenized, tgt_embed_list, tgt_url_list = \\\n",
    "            get_all_relevant_domain_data(chunk_list_tgt, base_domain_tgt, tgt_lang_code, embed_dict)\n",
    "        cd = CachedData(src_text_list_tokenized, src_embed_list, src_url_list, \n",
    "                        tgt_text_list_tokenized, tgt_embed_list, tgt_url_list)\n",
    "        lru_cache.put(base_domain_src, cd)\n",
    "    else:\n",
    "        print(\"cache hit\")\n",
    "    src_text_list_tokenized, src_embed_list, src_url_list = cd.get_src_data()\n",
    "    tgt_text_list_tokenized, tgt_embed_list, tgt_url_list = cd.get_tgt_data()\n",
    "    pca, lidf_weighter = cd.get_fitted_objects()\n",
    "    \n",
    "    #Handle negative augment data logic\n",
    "    if augment_negative_data_method is not None:\n",
    "        print(augment_negative_data_method, \"method\")\n",
    "        create_augmented_negative_sample(src_url, src_text_list_tokenized, src_url_list, src_embed_list,\n",
    "                            tgt_url, tgt_text_list_tokenized, tgt_url_list, tgt_embed_list,\n",
    "                            augment_negative_data_method)\n",
    "    \n",
    "    src_doc_embed = get_doc_embedding(src_url, src_text_list_tokenized, src_url_list, src_embed_list,\n",
    "                      lidf_weighter, pca, pert_obj, doc_vector_method)\n",
    "    tgt_doc_embed = get_doc_embedding(tgt_url, tgt_text_list_tokenized, tgt_url_list, tgt_embed_list,\n",
    "                      lidf_weighter, pca, pert_obj, doc_vector_method)\n",
    "    return src_doc_embed, tgt_doc_embed\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Datasets and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now defime dataloader class\n",
    "#from threading import Lock\n",
    "\n",
    "#NOTE: augment_negative_data_flag is for telling dataset class to slightly modify matching docs to form close, but neg pairs\n",
    "# This helps the sensitivity of the classifier\n",
    "\n",
    "#First get embed_dict, src_to_tgt_map, tgt_to_src_map\n",
    "#Then build pairset\n",
    "#Finally, at each idx, just get embed_src, embed_tgt, y\n",
    "class DocEmbedDataset(Dataset):\n",
    "    \n",
    "    \"\"\"\n",
    "    DocEmbeddingDataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, chunks_paths_list, src_lang, tgt_lang, doc_vector_method=\"SENT_ORDER\", cache_capacity=500): #NOTE: BE SURE TO ALLOCATE LOTS OF MEM\n",
    "      self.src_to_tgt_pairs, self.tgt_to_src_pairs = get_matching_url_dicts()\n",
    "      self.embed_dict = load_embed_dict(chunks_paths_list)\n",
    "      self.data_list = build_pair_dataset(self.embed_dict, self.src_to_tgt_pairs, self.tgt_to_src_pairs)\n",
    "      \n",
    "      self.src_lang = src_lang\n",
    "      self.tgt_lang = tgt_lang\n",
    "      \n",
    "      if doc_vector_method not in ['AVG', 'AVG_BP', 'SENT_ORDER']:\n",
    "        raise ValueError(\"\"\"Doc Vec Method must be one of the following:\n",
    "                            AVF, AVG_BP, SENT_ORDER\n",
    "                            Not found: %s\n",
    "                            \"\"\" % doc_vector_method)\n",
    "      self.doc_vector_method = doc_vector_method\n",
    "      self.pert_obj = ModifiedPertV2(None, None)\n",
    "      self.lru_cache = LRUCache(cache_capacity)\n",
    "      \n",
    "      self.exception_counter = 0\n",
    "      \n",
    "      #self.lock = Lock()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Get length of the dataset\n",
    "        \"\"\"\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self,\n",
    "                    idx):\n",
    "        \"\"\"\n",
    "        Gets the two vectors and target\n",
    "        \"\"\"\n",
    "        _, _, src_url, tgt_url, y_match_label, augment_negative_data_method = self.data_list[idx]\n",
    "        #print(\"CACHE LIST: %s \" %  list(self.lru_cache.keys())) #REMOVE AFTER DEBUG\n",
    "        #with self.lock:\n",
    "        #print(y_match_label, \"Y LABEL\", src_url, tgt_url, augment_negative_data_method)\n",
    "        '''src_doc_embedding, tgt_doc_embedding = load_embed_pairs(src_url, tgt_url, \n",
    "                                                                self.embed_dict,\n",
    "                                                                src_lang=self.src_lang,\n",
    "                                                                tgt_lang=self.tgt_lang)\n",
    "        src_doc_embedding, tgt_doc_embedding = src_doc_embedding[0], tgt_doc_embedding[0] '''\n",
    "        try:                                               \n",
    "          src_doc_embedding, tgt_doc_embedding = handle_doc_embed_logic(self.embed_dict,\n",
    "                                                                        src_url, tgt_url,\n",
    "                                                                        self.src_lang, self.tgt_lang,\n",
    "                                                                        self.doc_vector_method,\n",
    "                                                                        self.pert_obj,\n",
    "                                                                        self.lru_cache,\n",
    "                                                                        augment_negative_data_method)\n",
    "        except Exception as e:\n",
    "          print(e, \"DATA LOADING EXCEPTION, RETURNING NOISE TO BE USED AS NEG PAIR\")\n",
    "          self.exception_counter += 1\n",
    "          return np.random.random((2048)), np.random.random((2048)), 0 # In rare exception cases\n",
    "        return src_doc_embedding, tgt_doc_embedding, y_match_label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Dataset Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13020\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/dstambl2/doc_alignment_implementations/thompson_2021_doc_align/train_base_classifier.ipynb Cell 18'\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blogin2.clsp.jhu.edu/home/dstambl2/doc_alignment_implementations/thompson_2021_doc_align/train_base_classifier.ipynb#ch0000017vscode-remote?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(len_length)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blogin2.clsp.jhu.edu/home/dstambl2/doc_alignment_implementations/thompson_2021_doc_align/train_base_classifier.ipynb#ch0000017vscode-remote?line=5'>6</a>\u001b[0m particular_line  \u001b[39m=\u001b[39m linecache\u001b[39m.\u001b[39mgetline(\u001b[39m'\u001b[39m\u001b[39m/home/dstambl2/doc_alignment_implementations/data/cc_aligned_si_data/classifier_train_doc_vecs/valid/data.txt\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m13021\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Blogin2.clsp.jhu.edu/home/dstambl2/doc_alignment_implementations/thompson_2021_doc_align/train_base_classifier.ipynb#ch0000017vscode-remote?line=7'>8</a>\u001b[0m src, tgt, label \u001b[39m=\u001b[39m particular_line\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blogin2.clsp.jhu.edu/home/dstambl2/doc_alignment_implementations/thompson_2021_doc_align/train_base_classifier.ipynb#ch0000017vscode-remote?line=9'>10</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstr_float_list_to_np\u001b[39m(\u001b[39minput\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blogin2.clsp.jhu.edu/home/dstambl2/doc_alignment_implementations/thompson_2021_doc_align/train_base_classifier.ipynb#ch0000017vscode-remote?line=10'>11</a>\u001b[0m     \u001b[39m'''\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blogin2.clsp.jhu.edu/home/dstambl2/doc_alignment_implementations/thompson_2021_doc_align/train_base_classifier.ipynb#ch0000017vscode-remote?line=11'>12</a>\u001b[0m \u001b[39m    Helper function\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blogin2.clsp.jhu.edu/home/dstambl2/doc_alignment_implementations/thompson_2021_doc_align/train_base_classifier.ipynb#ch0000017vscode-remote?line=12'>13</a>\u001b[0m \u001b[39m    '''\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 1)"
     ]
    }
   ],
   "source": [
    "import linecache\n",
    "import json\n",
    "import numpy as np\n",
    "len_length = sum(1 for line in open('/home/dstambl2/doc_alignment_implementations/data/cc_aligned_si_data/classifier_train_doc_vecs/valid/data.txt'))\n",
    "print(len_length)\n",
    "particular_line  = linecache.getline('/home/dstambl2/doc_alignment_implementations/data/cc_aligned_si_data/classifier_train_doc_vecs/valid/data.txt', 13021)\n",
    "\n",
    "src, tgt, label = particular_line.split('\\t')\n",
    "\n",
    "def str_float_list_to_np(input):\n",
    "    '''\n",
    "    Helper function\n",
    "    '''\n",
    "    input = input.replace(']','').replace('[', '')\n",
    "    input = [float(item) for item in input.split(',')]\n",
    "    return np.asarray(input)\n",
    "\n",
    "src, tgt = str_float_list_to_np(src), str_float_list_to_np(tgt)\n",
    "\n",
    "print(src.shape, src.dtype, tgt.shape, tgt.dtype)\n",
    "print(len(src), src[:10], src[-10:], tgt[:10], tgt[-10:])\n",
    "#x = json.loads(src)\n",
    "\n",
    "#print(x[0:25])\n",
    "\n",
    "#src = np.frombuffer(src.encode(), dtype=float, count=-1)\n",
    "#tgt = np.frombuffer(tgt.encode(), dtype=float, count=-1)\n",
    "#label = int(label)\n",
    "\n",
    "#print(src.shape)\n",
    "#print(src.shape, label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: These classes just read in train set\n",
    "class DocEmbedDataset(Dataset):\n",
    "    \n",
    "    \"\"\"\n",
    "    DocEmbeddingDataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, file_path, src_lang, tgt_lang): #NOTE: BE SURE TO ALLOCATE LOTS OF MEM\n",
    "      self.src_lang = src_lang\n",
    "      self.tgt_lang = tgt_lang\n",
    "      self.file_path = file_path\n",
    "      \n",
    "      \n",
    "      self.file_len = sum(1 for line in open(file_path))\n",
    "      \n",
    "    \n",
    "    def __len__(self):\n",
    "      return self.file_len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "      \n",
    "      def str_float_list_to_np(input):\n",
    "        '''\n",
    "        Helper function\n",
    "        '''\n",
    "        input = input.replace(']','').replace('[', '')\n",
    "        input = [float(item) for item in input.split(',')]\n",
    "        return np.asarray(input)\n",
    "\n",
    "      line_cache_idx = idx + 1 #Line cache is indexed at 1 for some reason\n",
    "      data_record  = linecache.getline(self.file_path, line_cache_idx)\n",
    "      try:\n",
    "        src_raw, tgt_raw, label_str = data_record.split('\\t')\n",
    "      except Exception as e:\n",
    "        print(idx, data_record, \"EXCEPTION\")\n",
    "      src_doc_embedding, tgt_doc_embedding = str_float_list_to_np(src_raw), str_float_list_to_np(tgt_raw)\n",
    "      return src_doc_embedding, tgt_doc_embedding, int(label_str)\n",
    "\n",
    "    \n",
    "\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dataloader (USING NEW DATALOADER FORMAT)\n",
    "val_dataset= DocEmbedDataset('/home/dstambl2/doc_alignment_implementations/data/cc_aligned_si_data/classifier_train_doc_vecs/valid/data.txt', SRC_LANG_CODE, TGT_LANG_CODE)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=True) #prefetch_factor=5, prefetch maybe could help \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dataloader (USING OLDER DATALOADER FORMAT)\n",
    "train_dataset=DocEmbedDataset(chunks_paths_train, SRC_LANG_CODE, TGT_LANG_CODE) #TODO: Remove after done debug of loop\n",
    "validation_dataset=DocEmbedDataset(chunks_paths_val, SRC_LANG_CODE, TGT_LANG_CODE)\n",
    "test_dataset=DocEmbedDataset(chunks_paths_test, SRC_LANG_CODE, TGT_LANG_CODE)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True) #prefetch_factor=5, prefetch maybe could help \n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=1, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "407\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "#, len(validation_dataset), len(test_dataloader))\n",
    "for i, vals in enumerate(val_dataloader):\n",
    "    x_1, x_2, y = vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Plotting Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define loss func\n",
    "#Plot loss function\n",
    "def plot_loss_charts(train_loss_store, validation_loss_store):\n",
    "  '''\n",
    "  Plots loss charts over course of training\n",
    "  '''\n",
    "  ## Plotting epoch-wise test loss curve:\n",
    "  plt.plot(train_loss_store, '-o', label = 'train_loss', color = 'orange')\n",
    "  plt.plot(validation_loss_store, label = 'validation_loss', color = 'blue')\n",
    "  plt.xlabel('Epoch Number')\n",
    "  plt.ylabel('Loss At each epoch')\n",
    "  plt.legend()\n",
    "  plot_loss_charts\n",
    "  #plt.show()\n",
    "  plt.savefig('/home/dstambl2/doc_alignment_implementations/thompson_2021_doc_align/devtools/plots/%s_two.png' % \"loss_plots\")\n",
    "  plt.clf()\n",
    "\n",
    "\n",
    "def plot_accuracy(train_score_store, validation_score_store, skip_plot=False):\n",
    "  '''\n",
    "  Plots Accuracy charts over course of training\n",
    "  '''\n",
    "  #Don't plot if this flag is set to true\n",
    "  if skip_plot:\n",
    "    return\n",
    "\n",
    "  ## Plotting epoch-wise test loss curve:\n",
    "  plt.plot(train_score_store, '-o', label = 'train_accuracy', color = 'orange')\n",
    "  plt.plot(validation_score_store, label = 'validation_accuracy', color = 'blue')\n",
    "  plt.xlabel('Epoch Number')\n",
    "  plt.ylabel('Accuracy At each epoch')\n",
    "  plt.legend()\n",
    "  #plt.show()\n",
    "  plt.savefig('/home/dstambl2/doc_alignment_implementations/thompson_2021_doc_align/devtools/plots/%s_two.png' % \"acc_plots\")\n",
    "  plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocAlignerClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size_one=256, hidden_size_two=128, hidden_size_three=64):\n",
    "        super(DocAlignerClassifier, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size, hidden_size_one)\n",
    "        self.fc2 = nn.Linear(hidden_size_one, hidden_size_two)\n",
    "        self.fc3 = nn.Linear( hidden_size_two, hidden_size_three)\n",
    "        self.fc_out = nn.Linear(hidden_size_three, 1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        #Optional\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        #self.batchnorm1 = nn.BatchNorm1d(128)\n",
    "        #self.batchnorm2 = nn.BatchNorm1d(64)\n",
    "        \n",
    "        nn.init.kaiming_uniform_(self.fc1.weight) #He init for relu activation layers\n",
    "        nn.init.kaiming_uniform_(self.fc2.weight)\n",
    "        #nn.init.kaiming_uniform_(self.fc3.weight)\n",
    "        nn.init.xavier_uniform_(self.fc_out.weight) #Xavier init for sigmoid\n",
    "\n",
    "                \n",
    "    def forward(self, src_doc_embed, tgt_doc_embed):\n",
    "        '''\n",
    "        Forward data through the lstm\n",
    "        '''\n",
    "        combined_input = torch.cat((src_doc_embed.view(src_doc_embed.size(0), -1),\n",
    "                          tgt_doc_embed.view(tgt_doc_embed.size(0), -1)), dim=1)\n",
    "        #print(combined_input.shape, \"combed input\")\n",
    "\n",
    "        x = self.relu(self.fc1(combined_input))\n",
    "        #x = self.batchnorm1(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x)) #NOTE: Better to leave out batch norm\n",
    "        #x = self.batchnorm2(x)\n",
    "        x = self.dropout(x)\n",
    "        output = self.sigmoid(self.fc_out(x))\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Define Model Saving functionality\n",
    "import json\n",
    "def handle_model_save(val_acc_store, val_loss_store, train_loss_store, train_acc_store,\n",
    "                      base_path, model_name, epoch_num, model, optimizer):\n",
    "  '''\n",
    "  Function for saving models and training data\n",
    "  '''\n",
    "  save_dict = {\n",
    "      'val_acc_store': val_acc_store,\n",
    "      'val_loss_store': val_loss_store,\n",
    "      'train_loss_store': train_loss_store,\n",
    "      'train_acc_store': train_acc_store,\n",
    "      'epoch': epoch_num,\n",
    "  }\n",
    "  traing_info_path = \"%s/%s_epoch_%s.json\" % (base_path, model_name, epoch_num)\n",
    "  with open(traing_info_path, 'w') as f:\n",
    "    json.dump(save_dict, f)\n",
    "\n",
    "  MODEL_PATH = \"%s/%s_%s\" % (base_path, model_name, epoch_num)\n",
    "  torch.save({\n",
    "              'model_state_dict': model.state_dict(),\n",
    "              'optimizer_state_dict': optimizer.state_dict(),\n",
    "              }, MODEL_PATH)\n",
    "\n",
    "\n",
    "#Define loading historic training data func\n",
    "def load_train_data(traing_info_path):\n",
    "  with open(traing_info_path) as json_file:\n",
    "    data = json.load(json_file)\n",
    "  \n",
    "  return data['val_acc_store'], data['val_loss_store'], data['train_loss_store'], data['train_acc_store'], int(data['epoch'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Define Loss and Accuracy Eval Function\n",
    "def eval_acc_and_loss_func(model, loader, device, loss_metric, is_train = False, verbose = 1):\n",
    "    '''\n",
    "    Evaluate Function for CNN training\n",
    "    Slightly different than eval function from part 1\n",
    "    '''\n",
    "    correct, total, loss_sum = 0, 0, 0\n",
    "    \n",
    "    #temp_idx = 0  TODO: remove when down with loop debug\n",
    "    \n",
    "    eval_type = \"Train\" if is_train else \"Validation\"\n",
    "    for X_1, X_2, Y in loader:\n",
    "        outputs, predicted, calculated_loss = None, None, None\n",
    "        X_1, X_2, Y = X_1.to(device), X_2.to(device), Y.to(device)\n",
    "        \n",
    "        X_1, X_2 = X_1.float(), X_2.float()\n",
    "        outputs = model(X_1, X_2)\n",
    "        \n",
    "        #Reshape output and turn y into a float\n",
    "        outputs = outputs.view(-1)\n",
    "        Y = Y.float()\n",
    "        predicted = torch.round(outputs)\n",
    "        total += Y.size(0)\n",
    "        \n",
    "        correct += (predicted == Y).sum().item()\n",
    "        calculated_loss = loss_metric(outputs,Y).item()\n",
    "        loss_sum += calculated_loss\n",
    "        \n",
    "        #TODO: remove these lines when done with loop debug\n",
    "        #temp_idx += 1\n",
    "        #if temp_idx >= 3:\n",
    "        #    break\n",
    "        \n",
    "        \n",
    "    outputs, predicted, calculated_loss = None, None, None\n",
    "    if verbose:\n",
    "        print('%s accuracy: %f %%' % (eval_type, 100.0 * correct / total))\n",
    "        print('%s loss: %f' % (eval_type, loss_sum / total))\n",
    "    print\n",
    "    return 100.0 * correct / total, loss_sum/ total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINE TRAIN LOOP HERE\n",
    "def train(model,\n",
    "          optimizer,\n",
    "          loss_metric,\n",
    "          lr,\n",
    "          train_dataloader,\n",
    "          valid_dataloader,\n",
    "          device,\n",
    "          epochs=5,\n",
    "          stopping_threshold=3,\n",
    "          saving_per_epoch=1,\n",
    "          base_save_path=\"/home/dstambl2/doc_alignment_implementations/thompson_2021_doc_align/devtools/models\",\n",
    "          model_name=\"train_loop_model\",\n",
    "          load_train_hist_path=None,\n",
    "          **kwargs):\n",
    "    \"\"\"\n",
    "    For each epoch, loop through batch,\n",
    "    compute forward and backward passes, apply gradient updates\n",
    "    Evaluate results and output\n",
    "    \"\"\"\n",
    "\n",
    "    #If data already exists, that means the model was preloaded and training should\n",
    "    #resume from where it was interupted\n",
    "    if load_train_hist_path is not None:\n",
    "      val_acc_store, val_loss_store, train_loss_store, train_acc_store, start_epoch =load_train_data(load_train_hist_path)\n",
    "    else:\n",
    "      train_loss_store, train_acc_store = [], []\n",
    "      val_loss_store, val_acc_store, = [], []\n",
    "      start_epoch = 0\n",
    "\n",
    "    #Declare variables for early stopping\n",
    "    last_val_loss, stop_tracker = 100, 0\n",
    "\n",
    "    #training loop:\n",
    "    print(\"Starting Training\")\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "      time1 = time.time() #timekeeping\n",
    "      outputs, loss = None, None\n",
    "\n",
    "      model.train()\n",
    "      \n",
    "      correct_train, total, loss_sum = 0, 0, 0\n",
    "      for i, (x_1, x_2, y) in enumerate(train_dataloader):\n",
    "        \n",
    "        # Print device human readable names\n",
    "        #torch.cuda.get_device_name()\n",
    "\n",
    "        x_1, x_2, y = x_1.to(device), x_2.to(device), y.to(device)\n",
    "\n",
    "        #loss calculation and gradient update:\n",
    "\n",
    "        x_1, x_2 = x_1.float(), x_2.float()\n",
    "        if i > 0 or epoch > 0:\n",
    "          optimizer.zero_grad()\n",
    "        outputs = model.forward(x_1, x_2)\n",
    "        \n",
    "        #Reshape output and turn y into a float\n",
    "        outputs = outputs.view(-1)\n",
    "        y = y.float()\n",
    "        #print(y.shape, outputs.shape, outputs, \"Loss inp info\")\n",
    "\n",
    "        \n",
    "        loss = loss_metric(outputs, y)\n",
    "        loss.backward()\n",
    "                      \n",
    "        ##performing update:\n",
    "        optimizer.step()\n",
    "\n",
    "        #Update Loss Info\n",
    "        loss_sum += loss.item()\n",
    "        \n",
    "        #Acc was likely not increasing, because preds kept rounding to zero\n",
    "        predicted = torch.round(outputs) #NOTE: MODEL predictions keep rounding to zero\n",
    "        #print(outputs, predicted, y, \"predicted stuff\", y.size(0))\n",
    "\n",
    "        total += y.size(0)\n",
    "        correct_train += (predicted == y).sum().item()\n",
    "        \n",
    "        #TEMP, TODO: Delete when done debug\n",
    "        #if i >= 3:\n",
    "        #  break\n",
    "              \n",
    "      print(\"Epoch\",epoch+1,':')\n",
    "\n",
    "      \n",
    "      model.eval()\n",
    "      with torch.no_grad():\n",
    "\n",
    "        #Print Train Info\n",
    "        print('%s accuracy: %f %%' % (\"Train\", 100.0 * (correct_train / total)))\n",
    "        print('%s loss: %f' % (\"Train\", loss_sum / total))\n",
    "        print\n",
    "        \n",
    "        train_acc, train_loss = 100.0 * correct_train / total, loss_sum/ total\n",
    "        val_acc, val_loss = eval_acc_and_loss_func(model, valid_dataloader, device, loss_metric, is_train = False)\n",
    "\n",
    "        val_acc_store.append(val_acc)\n",
    "        val_loss_store.append(val_loss)\n",
    "\n",
    "        train_loss_store.append(train_loss)\n",
    "        train_acc_store.append(train_acc)\n",
    "\n",
    "      time2 = time.time() #timekeeping\n",
    "      #if show_progress:\n",
    "      print('Elapsed time for epoch:',time2 - time1,'s')\n",
    "      print('ETA of completion:',(time2 - time1)*(epochs - epoch - 1)/60,'minutes')\n",
    "      \n",
    "      if (epoch + 1) % saving_per_epoch == 0:\n",
    "        handle_model_save(val_acc_store, val_loss_store, train_loss_store, train_acc_store,\n",
    "                          base_save_path, model_name, epoch + 1, model, optimizer)\n",
    "        print(\"Model Copy Saved\")\n",
    "      print()\n",
    "\n",
    "\n",
    "      #Handle early stopping logic\n",
    "      if val_loss >= last_val_loss:\n",
    "            stop_tracker += 1\n",
    "            if stop_tracker >= stopping_threshold:\n",
    "                print('Early Stopping triggered, Convergence has occured')\n",
    "                plot_loss_charts(train_loss_store, val_loss_store)\n",
    "                plot_accuracy(train_acc_store, val_acc_store)\n",
    "                handle_model_save(val_acc_store, val_loss_store, train_loss_store, train_acc_store,\n",
    "                  base_save_path, model_name, epoch + 1, model, optimizer)\n",
    "                print(\"Model Copy Saved\")\n",
    "\n",
    "                return train_loss_store, val_acc_store\n",
    "      else:\n",
    "          stop_tracker = 0\n",
    "      last_val_loss = val_loss\n",
    "\n",
    "\n",
    "    plot_loss_charts(train_loss_store, val_loss_store)\n",
    "    plot_accuracy(train_acc_store, val_acc_store)\n",
    "    return train_loss_store, val_acc_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the model, and kick off training\n",
    "LEARNING_RATE = 0.001 #NOTE: CAN ADJUST\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = DocAlignerClassifier(4096) #Each doc vec is 2048, so times 2 will be 4096\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9)\n",
    "loss_fn = nn.BCELoss() #nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training\n",
      "1 Y LABEL https://www.etthawitthi.com/local/ranjith-de-zoysa-passed-away https://www.sinhala.etthawitthi.com/local/ranjith-de-zoysa-passed-away None\n",
      "0 Y LABEL http://www.sdzthl.com/galvalume-aluzinc-steel-coils-as-per-jis3321.html http://www.sdzthl.com/si/galvalume-aluzinc-steel-coils-as-per-jis3321.html sub\n",
      "sub method\n",
      "0 Y LABEL https://www.niceterminal.com/products/2-t-type-connection-terminal-block.html https://www.niceterminal.com/si/products/color-transparent-error-prevention-one-inlet-three-outlet.html None\n",
      "tensor([0.7942, 0.3255, 0.4217], grad_fn=<ViewBackward0>) tensor([1., 0., 0.], grad_fn=<RoundBackward0>) tensor([1., 0., 0.]) predicted stuff 3\n",
      "1 Y LABEL http://nirvanadhamma.lk/en/the-platform/dhamma-discourses/ http://www.nirvanadhamma.lk/the-platform/dhamma-discourses/ None\n",
      "cache hit\n",
      "1 Y LABEL http://xn--w0ct5a8c.xn--n0chiqomy9ed8bxb2a8e.xn--fzc2c9e2c/en/members-of-parliament/the-system-of-elections-in-sri-lanka/conduct-of-the-elections http://xn--w0ct5a8c.xn--n0chiqomy9ed8bxb2a8e.xn--fzc2c9e2c/si/members-of-parliament/the-system-of-elections-in-sri-lanka/conduct-of-the-elections None\n",
      "0 Y LABEL https://www.ceramictek.com/pepper-mill-part-kit/57064104.html https://www.ceramictek.com/si/zirconia-ceramic-tube/56668376.html None\n",
      "cache hit\n",
      "tensor([0.5368, 0.8181, 0.4331], grad_fn=<ViewBackward0>) tensor([1., 1., 0.], grad_fn=<RoundBackward0>) tensor([1., 1., 0.]) predicted stuff 3\n",
      "1 Y LABEL http://labour.gov.lk/web/index.php?option=com_org&letter=19&task=indexs&id=5&lang=en&Itemid= http://labour.gov.lk/web/index.php?option=com_org&letter=19&task=indexs&id=5&lang=si&Itemid= None\n",
      "1 Y LABEL http://www.roadmin.nw.gov.lk/roadmin/index.php?option=com_content&view=article&id=30&Itemid=69&lang=en http://www.roadmin.nw.gov.lk/roadmin/index.php?option=com_content&view=article&id=30&Itemid=69&lang=si None\n",
      "1 Y LABEL https://www.nfyo.com/products/steering-rack/lhd-steering-rack-core/fiat-lhd-steering-rack-core/ https://www.nfyo.com/si/products/steering-rack/lhd-steering-rack-core/fiat-lhd-steering-rack-core/ None\n",
      "tensor([0.7555, 0.1092, 0.6096], grad_fn=<ViewBackward0>) tensor([1., 0., 1.], grad_fn=<RoundBackward0>) tensor([1., 1., 1.]) predicted stuff 3\n",
      "0 Y LABEL http://tanzaniacrusher.org/xsm-hydraulic-cone-crusher/ http://tanzaniacrusher.org/si/xsm-hydraulic-cone-crusher/ add\n",
      "add method\n",
      "0 Y LABEL https://www.sliit.lk/2019/03/page/3/ https://www.sliit.lk/si/2019/03/page/3/ delete\n",
      "cache hit\n",
      "delete method\n",
      "1 Y LABEL http://www.coronationcasino.com/tag/boku-casino/ http://www.coronationcasino.com/si/tag/boku-casino/ None\n",
      "tensor([0.7485, 0.7134, 0.2111], grad_fn=<ViewBackward0>) tensor([1., 1., 0.], grad_fn=<RoundBackward0>) tensor([0., 0., 1.]) predicted stuff 3\n",
      "Epoch 1 :\n",
      "Train accuracy: 66.666667 %\n",
      "Train loss: 0.270488\n",
      "0 Y LABEL https://itstechschool.com/forgot-password/ https://itstechschool.com/si/forgot-password/ add\n",
      "cache hit\n",
      "add method\n",
      "0 Y LABEL http://www.yalanpack.com/transparent-tape/52423542.html https://www.yalanpack.com/si/productimage/44180374.html None\n",
      "cache hit\n",
      "0 Y LABEL http://www.manthri.lk/en/blog/posts/worst-performers-in-the-parliament-sept-2015-aug-2016 http://www.manthri.lk/si/blog/posts/worst-performers-in-the-parliament-sept-2015-aug-2016 add\n",
      "cache hit\n",
      "add method\n",
      "1 Y LABEL https://newlink.lk/en/links/newlink~all~electronics https://newlink.lk/si/links/newlink~all~electronics None\n",
      "cache hit\n",
      "1 Y LABEL http://www.zhitov.ru/en/gazon/ http://www.zhitov.ru/si/gazon/ None\n",
      "cache hit\n",
      "1 Y LABEL http://parliament.lk/en/forthcoming-business-of-the-house/view/1521?category=26 http://parliament.lk/si/forthcoming-business-of-the-house/view/1521?category=26 None\n",
      "cache hit\n",
      "1 Y LABEL http://museum.gov.lk/web/index.php?option=com_jevents&task=day.listevents&year=2019&month=05&day=26&catids=44&Itemid=110&lang=en http://museum.gov.lk/web/index.php?option=com_jevents&task=day.listevents&year=2019&month=05&day=26&catids=44&Itemid=110&lang=si None\n",
      "cache hit\n",
      "1 Y LABEL https://en.suenee.cz/tag/antikythera/ https://si.suenee.cz/tag/antikythera/ None\n",
      "cache hit\n",
      "0 Y LABEL http://www.gsmb.gov.lk/web/index.php?option=com_content&task=view&id=214&Itemid=79&lang=en http://www.gsmb.gov.lk/web/index.php?option=com_content&task=view&id=205&Itemid=79&lang=si None\n",
      "cache hit\n",
      "Validation accuracy: 44.444444 %\n",
      "Validation loss: 0.233826\n",
      "Elapsed time for epoch: 172.23138236999512 s\n",
      "ETA of completion: 11.482092157999675 minutes\n",
      "Model Copy Saved\n",
      "\n",
      "1 Y LABEL http://www.sltnet.lk/personal/promotions-and-offers https://www.sltnet.lk/si/personal/promotions-and-offers None\n",
      "0 Y LABEL http://www.rikoooo.com/en/register https://www.rikoooo.com/si/register sub\n",
      "sub method\n",
      "0 Y LABEL http://www.prda.sp.gov.lk/index.php?option=com_content&view=frontpage&Itemid=1&lang=en http://www.prda.sp.gov.lk/index.php?option=com_content&view=article&id=123%3Asapiri-gamak-supiri-ratak&catid=81%3Alatest-news&Itemid=50&lang=si None\n",
      "tensor([0.2449, 0.6970, 0.5778], grad_fn=<ViewBackward0>) tensor([0., 1., 1.], grad_fn=<RoundBackward0>) tensor([1., 0., 0.]) predicted stuff 3\n",
      "1 Y LABEL http://www.immigration.gov.lk/web/index.php?option=com_content&view=article&id=298:spokesman-of-department-of-immigration-and-emigration&catid=1:latest-news&Itemid=180&lang=en http://www.immigration.gov.lk/web/index.php?option=com_content&view=article&id=298:spokesman-of-department-of-immigration-and-emigration&catid=1:latest-news&Itemid=180&lang=si None\n",
      "cache hit\n",
      "1 Y LABEL https://www.seylan.lk/seylan-avurudu-campaign.html https://www.seylan.lk/si/seylan-avurudu-campaign.html None\n",
      "0 Y LABEL http://www.tjcylr.com/farm-irrigation-system-2.html http://www.tjcylr.com/si/32mm-soft-hose-with-good-price.html None\n",
      "tensor([0.1162, 0.9737, 0.4446], grad_fn=<ViewBackward0>) tensor([0., 1., 0.], grad_fn=<RoundBackward0>) tensor([1., 1., 0.]) predicted stuff 3\n",
      "1 Y LABEL http://www.manthri.lk/en/blog/posts/--30 http://www.manthri.lk/si/blog/posts/--30 None\n",
      "0 Y LABEL http://www.ideabeam.com/mobile/samsung-galaxy-j6-64gb-price.html http://si.ideabeam.com/mobile/samsung-galaxy-j6-64gb-price.html delete\n",
      "delete method\n",
      "0 Y LABEL https://www.buyaas.com/product/521-18-6/ https://si.buyaas.com/product/521-18-6/ add\n",
      "add method\n",
      "tensor([0.1942, 0.8155, 0.6379], grad_fn=<ViewBackward0>) tensor([0., 1., 1.], grad_fn=<RoundBackward0>) tensor([1., 0., 0.]) predicted stuff 3\n",
      "1 Y LABEL http://airforce.lk/tamil_news.php?news=2361 http://www.airforce.lk/sinhala/tamil_news.php?news=2361 None\n",
      "0 Y LABEL https://www.jxmachinery.com/gt4-14-wire-rod-rebar-straightening-cutting-machine.html https://www.jxmachinery.com/si/products/bending-center-machine None\n",
      "0 Y LABEL http://xn--w0ct5a8c.xn--n0chiqomy9ed8bxb2a8e.xn--fzc2c9e2c/en/deputy-chairman-of-committees http://xn--w0ct5a8c.xn--n0chiqomy9ed8bxb2a8e.xn--fzc2c9e2c/si/deputy-chairman-of-committees sub\n",
      "cache hit\n",
      "sub method\n",
      "tensor([0.0821, 0.5211, 0.6593], grad_fn=<ViewBackward0>) tensor([0., 1., 1.], grad_fn=<RoundBackward0>) tensor([1., 0., 0.]) predicted stuff 3\n",
      "Epoch 2 :\n",
      "Train accuracy: 16.666667 %\n",
      "Train loss: 0.413540\n",
      "0 Y LABEL https://itstechschool.com/forgot-password/ https://itstechschool.com/si/forgot-password/ add\n",
      "cache hit\n",
      "add method\n",
      "0 Y LABEL http://www.yalanpack.com/transparent-tape/52423542.html https://www.yalanpack.com/si/productimage/44180374.html None\n",
      "cache hit\n",
      "0 Y LABEL http://www.manthri.lk/en/blog/posts/worst-performers-in-the-parliament-sept-2015-aug-2016 http://www.manthri.lk/si/blog/posts/worst-performers-in-the-parliament-sept-2015-aug-2016 add\n",
      "cache hit\n",
      "add method\n",
      "1 Y LABEL https://newlink.lk/en/links/newlink~all~electronics https://newlink.lk/si/links/newlink~all~electronics None\n",
      "cache hit\n",
      "1 Y LABEL http://www.zhitov.ru/en/gazon/ http://www.zhitov.ru/si/gazon/ None\n",
      "cache hit\n",
      "1 Y LABEL http://parliament.lk/en/forthcoming-business-of-the-house/view/1521?category=26 http://parliament.lk/si/forthcoming-business-of-the-house/view/1521?category=26 None\n",
      "cache hit\n",
      "1 Y LABEL http://museum.gov.lk/web/index.php?option=com_jevents&task=day.listevents&year=2019&month=05&day=26&catids=44&Itemid=110&lang=en http://museum.gov.lk/web/index.php?option=com_jevents&task=day.listevents&year=2019&month=05&day=26&catids=44&Itemid=110&lang=si None\n",
      "cache hit\n",
      "1 Y LABEL https://en.suenee.cz/tag/antikythera/ https://si.suenee.cz/tag/antikythera/ None\n",
      "cache hit\n",
      "0 Y LABEL http://www.gsmb.gov.lk/web/index.php?option=com_content&task=view&id=214&Itemid=79&lang=en http://www.gsmb.gov.lk/web/index.php?option=com_content&task=view&id=205&Itemid=79&lang=si None\n",
      "cache hit\n",
      "Validation accuracy: 44.444444 %\n",
      "Validation loss: 0.242631\n",
      "Elapsed time for epoch: 267.1091537475586 s\n",
      "ETA of completion: 13.35545768737793 minutes\n",
      "Model Copy Saved\n",
      "\n",
      "1 Y LABEL http://sp.gov.lk/index.php?option=com_jevents&task=day.listevents&year=2017&month=01&day=13&Itemid=1&lang=en http://sp.gov.lk/index.php?option=com_jevents&task=day.listevents&year=2017&month=01&day=13&Itemid=1&lang=si None\n",
      "cache hit\n",
      "0 Y LABEL https://www.the-tailoress.com/tut_category/skirts/ https://www.the-tailoress.com/si/tut_category/skirts/ sub\n",
      "sub method\n",
      "1 Y LABEL https://microbit.org/en/2018-07-31-kcl-research/ https://microbit.org/si/2018-07-31-kcl-research/ None\n",
      "tensor([0.6354, 0.7718, 0.3491], grad_fn=<ViewBackward0>) tensor([1., 1., 0.], grad_fn=<RoundBackward0>) tensor([1., 0., 1.]) predicted stuff 3\n",
      "1 Y LABEL http://sppc.lk/index.php?option=com_jevents&task=day.listevents&year=2018&month=03&day=19&Itemid=0&lang=en http://www.sppc.lk/index.php?option=com_jevents&task=day.listevents&year=2018&month=03&day=19&Itemid=0&lang=si None\n",
      "1 Y LABEL https://it.bisnzz.com/global/Sicily/San-Filippo-del-Mela-c2432t4304173/list/Other%20Services%20-%20Public%20Places-7 https://it.bisnzz.com/global/Sicily/San-Filippo-del-Mela-c2432t4304173/list/Other%20Services%20-%20Public%20Places-7?lang=si None\n",
      "0 Y LABEL http://towerhall.lk/index.php?option=com_artistdirectory&Itemid=98&lang=en&limitstart=480 http://towerhall.lk/index.php?option=com_content&view=article&id=53&Itemid=56&lang=si None\n",
      "tensor([0.7838, 0.4130, 0.3418], grad_fn=<ViewBackward0>) tensor([1., 0., 0.], grad_fn=<RoundBackward0>) tensor([1., 1., 0.]) predicted stuff 3\n",
      "0 Y LABEL http://hzhinew.com/ http://www.hzhinew.com/si/about-us/factoryenvironment/ None\n",
      "1 Y LABEL http://www.chaoyucabinet.com/network-racks-cyac-02-series.html http://www.chaoyucabinet.com/si/network-racks-cyac-02-series.html None\n",
      "1 Y LABEL http://www.holmbygden.se/en/2014/03/12/hsk-premiar-i-varmecupen/ http://www.holmbygden.se/si/2014/03/12/hsk-premiar-i-varmecupen/ None\n",
      "tensor([0.6095, 0.5757, 0.3148], grad_fn=<ViewBackward0>) tensor([1., 1., 0.], grad_fn=<RoundBackward0>) tensor([0., 1., 1.]) predicted stuff 3\n",
      "0 Y LABEL http://www.yauwanabhimana-hnb.lk/ http://www.yauwanabhimana-hnb.lk/si add\n",
      "add method\n",
      "0 Y LABEL http://www.magiccar168.com/products/food-truck/ http://www.magiccar168.com/si/ None\n",
      "0 Y LABEL http://ukraine.admission.center/admission-procedure/ http://ukraine.admission.center/si/admission-procedure/ delete\n",
      "delete method\n",
      "tensor([0.1860, 0.4102, 0.8387], grad_fn=<ViewBackward0>) tensor([0., 0., 1.], grad_fn=<RoundBackward0>) tensor([0., 0., 0.]) predicted stuff 3\n",
      "Epoch 3 :\n",
      "Train accuracy: 50.000000 %\n",
      "Train loss: 0.270466\n",
      "0 Y LABEL https://itstechschool.com/forgot-password/ https://itstechschool.com/si/forgot-password/ add\n",
      "cache hit\n",
      "add method\n",
      "0 Y LABEL http://www.yalanpack.com/transparent-tape/52423542.html https://www.yalanpack.com/si/productimage/44180374.html None\n",
      "cache hit\n",
      "0 Y LABEL http://www.manthri.lk/en/blog/posts/worst-performers-in-the-parliament-sept-2015-aug-2016 http://www.manthri.lk/si/blog/posts/worst-performers-in-the-parliament-sept-2015-aug-2016 add\n",
      "cache hit\n",
      "add method\n",
      "1 Y LABEL https://newlink.lk/en/links/newlink~all~electronics https://newlink.lk/si/links/newlink~all~electronics None\n",
      "cache hit\n",
      "1 Y LABEL http://www.zhitov.ru/en/gazon/ http://www.zhitov.ru/si/gazon/ None\n",
      "cache hit\n",
      "1 Y LABEL http://parliament.lk/en/forthcoming-business-of-the-house/view/1521?category=26 http://parliament.lk/si/forthcoming-business-of-the-house/view/1521?category=26 None\n",
      "cache hit\n",
      "1 Y LABEL http://museum.gov.lk/web/index.php?option=com_jevents&task=day.listevents&year=2019&month=05&day=26&catids=44&Itemid=110&lang=en http://museum.gov.lk/web/index.php?option=com_jevents&task=day.listevents&year=2019&month=05&day=26&catids=44&Itemid=110&lang=si None\n",
      "cache hit\n",
      "1 Y LABEL https://en.suenee.cz/tag/antikythera/ https://si.suenee.cz/tag/antikythera/ None\n",
      "cache hit\n",
      "0 Y LABEL http://www.gsmb.gov.lk/web/index.php?option=com_content&task=view&id=214&Itemid=79&lang=en http://www.gsmb.gov.lk/web/index.php?option=com_content&task=view&id=205&Itemid=79&lang=si None\n",
      "cache hit\n",
      "Validation accuracy: 44.444444 %\n",
      "Validation loss: 0.251265\n",
      "Elapsed time for epoch: 184.2566773891449 s\n",
      "ETA of completion: 6.14188924630483 minutes\n",
      "Model Copy Saved\n",
      "\n",
      "1 Y LABEL https://fbion.com/en/how-to-change-background-facebook.html https://fbion.com/si/how-to-change-background-facebook.html None\n",
      "1 Y LABEL https://church-of-christ.org/cb-profile/16159-usertcp8lvfkpwvc6j8h.html https://church-of-christ.org/si/cb-profile/16159-usertcp8lvfkpwvc6j8h.html None\n",
      "cache hit\n",
      "1 Y LABEL http://www.webdevlinux.com/html-tutorial/html-standards-syntax.php http://si.webdevlinux.com/html-tutorial/html-standards-syntax.php None\n",
      "tensor([0.3889, 0.7806, 0.5225], grad_fn=<ViewBackward0>) tensor([0., 1., 1.], grad_fn=<RoundBackward0>) tensor([1., 1., 1.]) predicted stuff 3\n",
      "1 Y LABEL https://www.ceramictek.com/structural-ceramic-component/47095007.html https://www.ceramictek.com/si/structural-ceramic-component/47095007.html None\n",
      "cache hit\n",
      "0 Y LABEL https://www.ideabeam.com/tablet/brand/vivo/ https://si.ideabeam.com/tablet/brand/vivo/ delete\n",
      "cache hit\n",
      "delete method\n",
      "1 Y LABEL http://museum.gov.lk/web/index.php?option=com_jevents&task=icalrepeat.detail&evid=490&Itemid=110&year=2014&month=06&day=27&uid=02c684a53b79990bd45f210e90a5fd71&catids=44&lang=en http://museum.gov.lk/web/index.php?option=com_jevents&task=icalrepeat.detail&evid=490&Itemid=110&year=2014&month=06&day=27&uid=02c684a53b79990bd45f210e90a5fd71&catids=44&lang=si None\n",
      "tensor([0.0778, 0.7165, 0.8457], grad_fn=<ViewBackward0>) tensor([0., 1., 1.], grad_fn=<RoundBackward0>) tensor([1., 0., 1.]) predicted stuff 3\n",
      "1 Y LABEL http://www.zhengmaoelec.com/zm86-2r110l-800l.html http://www.zhengmaoelec.com/si/zm86-2r110l-800l.html None\n",
      "1 Y LABEL https://www.kehaisonic.com/products/ultrasonic-food-cutting/ https://www.kehaisonic.com/si/products/ultrasonic-food-cutting/ None\n",
      "1 Y LABEL http://www.ovit-ks.com/products/sink/ http://www.ovit-ks.com/si/products/sink/ None\n",
      "tensor([0.2824, 0.8938, 0.1359], grad_fn=<ViewBackward0>) tensor([0., 1., 0.], grad_fn=<RoundBackward0>) tensor([1., 1., 1.]) predicted stuff 3\n",
      "0 Y LABEL https://www.bestforexeas.com/ghost-trader-ea-review/ https://www.bestforexeas.com/si/adxvma-histo-final-indicator/ None\n",
      "0 Y LABEL http://www.restart-industry.com/hot-sale-cheap-red-canvas-espadrills-casual-shoes.html http://www.restart-industry.com/si/faqs/ None\n",
      "0 Y LABEL http://msn-chatting.com/watch-girls-on-cam/ http://msn-chatting.com/si/webcams/webcam-girls/ None\n",
      "tensor([0.8677, 0.3133, 0.5633], grad_fn=<ViewBackward0>) tensor([1., 0., 1.], grad_fn=<RoundBackward0>) tensor([0., 0., 0.]) predicted stuff 3\n",
      "Epoch 4 :\n",
      "Train accuracy: 41.666667 %\n",
      "Train loss: 0.345059\n",
      "0 Y LABEL https://itstechschool.com/forgot-password/ https://itstechschool.com/si/forgot-password/ add\n",
      "cache hit\n",
      "add method\n",
      "0 Y LABEL http://www.yalanpack.com/transparent-tape/52423542.html https://www.yalanpack.com/si/productimage/44180374.html None\n",
      "cache hit\n",
      "0 Y LABEL http://www.manthri.lk/en/blog/posts/worst-performers-in-the-parliament-sept-2015-aug-2016 http://www.manthri.lk/si/blog/posts/worst-performers-in-the-parliament-sept-2015-aug-2016 add\n",
      "cache hit\n",
      "add method\n",
      "1 Y LABEL https://newlink.lk/en/links/newlink~all~electronics https://newlink.lk/si/links/newlink~all~electronics None\n",
      "cache hit\n",
      "1 Y LABEL http://www.zhitov.ru/en/gazon/ http://www.zhitov.ru/si/gazon/ None\n",
      "cache hit\n",
      "1 Y LABEL http://parliament.lk/en/forthcoming-business-of-the-house/view/1521?category=26 http://parliament.lk/si/forthcoming-business-of-the-house/view/1521?category=26 None\n",
      "cache hit\n",
      "1 Y LABEL http://museum.gov.lk/web/index.php?option=com_jevents&task=day.listevents&year=2019&month=05&day=26&catids=44&Itemid=110&lang=en http://museum.gov.lk/web/index.php?option=com_jevents&task=day.listevents&year=2019&month=05&day=26&catids=44&Itemid=110&lang=si None\n",
      "cache hit\n",
      "1 Y LABEL https://en.suenee.cz/tag/antikythera/ https://si.suenee.cz/tag/antikythera/ None\n",
      "cache hit\n",
      "0 Y LABEL http://www.gsmb.gov.lk/web/index.php?option=com_content&task=view&id=214&Itemid=79&lang=en http://www.gsmb.gov.lk/web/index.php?option=com_content&task=view&id=205&Itemid=79&lang=si None\n",
      "cache hit\n",
      "Validation accuracy: 44.444444 %\n",
      "Validation loss: 0.254487\n",
      "Elapsed time for epoch: 137.93054223060608 s\n",
      "ETA of completion: 2.2988423705101013 minutes\n",
      "Model Copy Saved\n",
      "\n",
      "Early Stopping triggered, Convergence has occured\n",
      "Model Copy Saved\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#TODO: Test this with just base sent embeddings (as a place holder) just to see that training is working\n",
    "# Since its much faster than using doc vector\n",
    "train(\n",
    "        model,\n",
    "        optimizer,\n",
    "        loss_fn,\n",
    "        LEARNING_RATE,\n",
    "        train_dataloader,\n",
    "        validation_dataloader,\n",
    "        device,\n",
    "        epochs=5\n",
    "    )\n",
    "print(train_dataset.exception_counter, \"exception counter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eval on test set\n",
    "#model_checkpoint = torch.load('/home/dstambl2/doc_alignment_implementations/thompson_2021_doc_align/devtools/models/train_loop_model_5')\n",
    "#model.load_state_dict(model_checkpoint['model_state_dict'])\n",
    "eval_acc_and_loss_func(model, test_dataloader, device, loss_fn, is_train = False, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
